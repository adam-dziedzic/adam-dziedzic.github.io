@inproceedings{dziedzic2014analysis,
    title = {Analysis and comparison of NoSQL databases with an introduction to consistent references in Big Data storage systems},
    author = {Dziedzic, Adam and Mulawka, Jan},
    booktitle = {Photonics Applications in Astronomy, Communications, Industry, and High-Energy Physics Experiments 2014},
    volume = {9290},
    pages = {92902V},
    year = {2014},
    organization = {International Society for Optics and Photonics}
}

@inproceedings{dziedzic2015bigdawg,
    title = {BigDAWG: a Polystore for Diverse Interactive Applications},
    author = {Dziedzic, Adam and Duggan, Jennie and Elmore, Aaron J. and Gadepally, Vijay and Stonebraker, Michael},
    booktitle = {IEEE Viz Data Systems for Interactive Analysis (DSIA)},
    year = {2015}
}

@inproceedings{meehan2016integrating,
    title = {Integrating Real-Time and Batch Processing in a Polystore},
    author = {Meehan, John and Zdonik, Stan and Tian, Shaobo and Tian, Yulong and Tatbul, Nesime and Dziedzic, Adam and Elmore, Aaron},
    booktitle = {HPEC 2016},
    year = {2016}
}

@inproceedings{dziedzic2016data,
    title = {Data Transformation and Migration in Polystores},
    author = {Dziedzic, Adam and Elmore, Aaron and Stonebraker, Michael},
    booktitle = {HPEC 2016},
    year = {2016},
    organization = {IEEE}
}

@inproceedings{dziedzic2016dbms,
    title = {DBMS Data Loading: An Analysis on Modern Hardware},
    author = {Dziedzic, Adam and Karpathiotakis, Manos and Alagiannis, Ioannis and Appuswamy, Raja and Ailamaki, Anastasia},
    booktitle = {ADMS 2016},
    year = {2016}
}

@inproceedings{mattson2017demonstrating,
    title = {Demonstrating the BigDAWG Polystore System for Ocean Metagenomics Analysis.},
    author = {Mattson, Tim and Gadepally, Vijay and She, Zuohao and Dziedzic, Adam and Parkhurst, Jeff},
    booktitle = {CIDR},
    year = {2017}
}

@article{obrien2017bigdawg,
    title = {Bigdawg polystore release and demonstration},
    author = {OBrien, Kyle and Gadepally, Vijay and Duggan, Jennie and Dziedzic, Adam and Elmore, Aaron and Kepner, Jeremy and Madden, Samuel and Mattson, Tim and She, Zuohao and Stonebraker, Michael},
    journal = {arXiv preprint arXiv:1701.05799},
    year = {2017}
}

@article{gadepally2017version,
    title = {Version 0.1 of the bigdawg polystore system},
    author = {Gadepally, Vijay and OBrien, Kyle and Dziedzic, Adam and Elmore, Aaron and Kepner, Jeremy and Madden, Samuel and Mattson, Tim and Rogers, Jennie and She, Zuohao and Stonebraker, Michael},
    journal = {arXiv preprint arXiv:1707.00721},
    year = {2017}
}

@inproceedings{gadepally2017bigdawg,
    title = {BigDAWG version 0.1},
    author = {Gadepally, Vijay and O'Brien, Kyle and Dziedzic, Adam and Elmore, Aaron and Kepner, Jeremy and Madden, Samuel and Mattson, Tim and Rogers, Jennie and She, Zuohao and Stonebraker, Michael},
    booktitle = {2017 IEEE High Performance Extreme Computing Conference (HPEC)},
    pages = {1--7},
    year = {2017},
    organization = {IEEE}
}

@article{dziedzic2017data,
    title = {Data Loading, Transformation and Migration for Database Management Systems},
    author = {Dziedzic, Adam},
    year = {2017},
    publisher = {The University of Chicago},
    pub_type = {Thesis},
}

@article{gadepallyseptember,
    title = {September 2017. BigDAWG Version 0.1},
    author = {Gadepally, V and O'Brien, K and Dziedzic, A and Elmore, A and Kepner, J and Madden, S and Mattson, T and Rogers, J and She, Z and Stonebraker, M},
    journal = {IEEE High Performance Extreme}
}

@inproceedings{dziedzic2018columnstore,
    title = {Columnstore and B+ tree-Are Hybrid Physical Designs Important?},
    author = {Dziedzic, Adam and Wang, Jingjing and Das, Sudipto and Ding, Bolin and Narasayya, Vivek R and Syamala, Manoj},
    booktitle = {Proceedings of the 2018 International Conference on Management of Data (SIGMOD 2018)},
    pages = {177--190},
    year = {2018},
    organization = {ACM},
    abstract = {Commercial DBMSs, such as Microsoft SQL Server, cater to diverse workloads including transaction processing, decision support, and operational analytics. They also support variety in physical design structures such as B+ tree and columnstore. The benefits of B+tree for OLTP workloads and columnstore for decision support workloads are well-understood. However, the importance of hybrid physical designs, consisting of both columnstore and B+ tree indexes on the same database, is not well-studied â€” a focus of this paper. We first quantify the trade-offs using carefully-crafted micro-benchmarks. This micro-benchmarking indicates that hybrid physical designs can result in orders of magnitude better performance depending on the workload. For complex real-world applications, choosing an appropriate combination of columnstore and B+ tree indexes for a database workload is challenging. We extend the Database Engine Tuning Advisor for Microsoft SQL Server to recommend a suitable combination of B+ tree and columnstore indexes for a given workload. Through extensive experiments using industry-standard benchmarks and several real-world customer workloads, we quantify how a physical design tool capable of recommending hybrid physical designs can result in orders of magnitude better execution costs compared to approaches that rely either on columnstore-only or B+ tree-only designs.}
}

@article{krishnan2018deeplens,
    title = {Deeplens: Towards a visual data management system},
    author = {Krishnan, Sanjay and Dziedzic, Adam and Elmore, Aaron J},
    journal = {CIDR},
    year = {2018}
}

@inproceedings{dziedzic2019band,
    title = {Band-limited Training and Inference for Convolutional Neural Networks},
    author = {Dziedzic, Adam and Paparizzos, Ioannis and Krishnan, Sanjay and Elmore, Aaron and Franklin, Michael},
    booktitle = {ICML (International Conference on Machine Learning)},
    year = {2019}
}

@article{krishnan2019artificial,
    title = {Artificial intelligence in resource-constrained and shared environments},
    author = {Krishnan, Sanjay and Elmore, Aaron J and Franklin, Michael and Paparrizos, John and Shang, Zechao and Dziedzic, Adam and Liu, Rui},
    journal = {ACM SIGOPS Operating Systems Review},
    volume = {53},
    number = {1},
    pages = {1--6},
    year = {2019},
    publisher = {ACM New York, NY, USA}
}

@article{dziedzic2019perturbation,
    title = {A Perturbation Analysis of Input Transformations for Adversarial Attacks},
    author = {Dziedzic, Adam and Krishnan, Sanjay},
    year = {2019}
}

@inproceedings{sathya2020machine,
    title = {Machine Learning based detection of multiple Wi-Fi BSSs for LTE-U CSAT},
    author = {Sathya, Vanlin and Dziedzic, Adam and Ghosh, Monisha and Krishnan, Sanjay},
    booktitle = {International Conference on Computing, Networking and Communications (ICNC 2020)},
    year = {2020},
    organization = {IEEE}
}

@article{dziedzic2020empirical,
    title = {An Empirical Evaluation of Perturbation-based Defenses},
    author = {Dziedzic, Adam and Krishnan, Sanjay},
    journal = {arXiv preprint arXiv:2002.03080},
    year = {2020}
}

@article{dziedzic2020machine,
    title = {Machine Learning enabled Spectrum Sharing in Dense LTE-U/Wi-Fi Coexistence Scenarios},
    author = {Dziedzic, Adam and Sathya, Vanlin and Rochman, Muhammad and Ghosh, Monisha and Krishnan, Sanjay},
    journal = {IEEE Open Journal of Vehicular Technology},
    year = {2020},
    publisher = {IEEE}
}

@article{dziedzic2020input,
    title = {Input and Model Compression for Adaptive and Robust Neural Networks},
    author = {Dziedzic, Adam},
    year = {2020},
    publisher = {The University of Chicago},
    pub_type = {Thesis},
}

@inproceedings{hendrycks-etal-2020-pretrained,
    title = "Pretrained Transformers Improve Out-of-Distribution Robustness",
    author = "Hendrycks, Dan  and
      Liu, Xiaoyuan  and
      Wallace, Eric  and
      Dziedzic, Adam  and
      Krishnan, Rishabh  and
      Song, Dawn",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.244",
    doi = "10.18653/v1/2020.acl-main.244",
    pages = "2744--2751",
    abstract = "Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers{'} performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.",
}

@inproceedings{capc2021iclr,
    title = {CaPC Learning: Confidential and Private Collaborative Learning},
    author = {Christopher A. Choquette-Choo and Natalie Dullerud and Adam Dziedzic and Yunxiang Zhang and Somesh Jha and Nicolas Papernot and Xiao Wang},
    booktitle = {International Conference on Learning Representations},
    year = {2021},
    url = {https://openreview.net/forum?id=h2EbJ4_wMVq},
    abstract = {Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties.},
    video_url = {https://www.youtube.com/watch?v=0yWeY39cPLo},
    code_url = {https://github.com/cleverhans-lab/capc-demo},
    blog_url = {http://www.cleverhans.io/2021/05/01/capc.html},
    slides_url = {https://adam-dziedzic.com/static/assets/slides/capc2.pdf},
}

@article{wong2021ML,
    author = {Wong, Arnold Y. L. and Harada, Garrett and Lee, Remy and Gandhi, Sapan D. and Dziedzic, Adam and Espinoza-Orias, Alejandro and Parnianpour, Mohamad and Louie, Philip K. and Basques, Bryce and An, Howard S. and Samartzis, Dino},
    title = {Preoperative paraspinal neck muscle characteristics predict early onset adjacent segment degeneration in anterior cervical fusion patients: A machine-learning modeling analysis},
    journal = {Journal of Orthopaedic Research},
    volume = {39},
    number = {8},
    pages = {1732-1744},
    keywords = {adjacent segment, cervical, degeneration, disc, disease, muscles, paraspinal, spine},
    doi = {https://doi.org/10.1002/jor.24829},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jor.24829},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jor.24829},
    abstract = {Abstract Early onset adjacent segment degeneration (ASD) can be found within six months after anterior cervical discectomy and fusion (ACDF). Deficits in deep paraspinal neck muscles may be related to early onset ASD. This study aimed to determine whether the morphometry of preoperative deep neck muscles (multifidus and semispinalis cervicis) predicted early onset ASD in patients with ACDF. Thirty-two cases of early onset ASD after a two-level ACDF and 30 matched non-ASD cases were identified from a large-scale cohort. The preoperative total cross-sectional area (CSA) of bilateral deep neck muscles and the lean muscle CSAs from C3 to C7 levels were measured manually on T2-weighted magnetic resonance imaging. Paraspinal muscle CSA asymmetry at each level was calculated. A support vector machine (SVM) algorithm was used to identify demographic, radiographic, and/or muscle parameters that predicted proximal/distal ASD development. No significant between-group differences in demographic or preoperative radiographic data were noted (mean age: 52.4â€‰Â±â€‰10.9 years). ACDFs comprised C3 to C5 (nâ€‰=â€‰9), C4 to C6 (nâ€‰=â€‰20), and C5 to C7 (nâ€‰=â€‰32) cases. Eighteen, eight, and six patients had proximal, distal, or both ASD, respectively. The SVM model achieved high accuracy (96.7\%) and an area under the curve (AUCâ€‰=â€‰0.97) for predicting early onset ASD. Asymmetry of fat at C5 (coefficient: 0.06), and standardized measures of C7 lean (coefficient: 0.05) and total CSA measures (coefficient: 0.05) were the strongest predictors of early onset ASD. This is the first study to show that preoperative deep neck muscle CSA, composition, and asymmetry at C5 to C7 independently predicted postoperative early onset ASD in patients with ACDF. Paraspinal muscle assessments are recommended to identify high-risk patients for personalized intervention.},
    year = {2021}
}

@misc{travers2021exploitability,
      title={On the Exploitability of Audio Machine Learning Pipelines to Surreptitious Adversarial Examples},
      author={Adelin Travers and Lorna Licollari and Guanghan Wang and Varun Chandrasekaran and Adam Dziedzic and David Lie and Nicolas Papernot},
      year={2021},
      eprint={2108.02010},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}


