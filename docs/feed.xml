<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Adam Dziedzic</title>
    <description>Adam&apos;s home page.</description>
    <link>https://adam-dziedzic.github.io//</link>
    <atom:link href="https://adam-dziedzic.github.io//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 19 Jan 2024 14:14:26 +0100</pubDate>
    <lastBuildDate>Fri, 19 Jan 2024 14:14:26 +0100</lastBuildDate>
    <generator>Jekyll v4.3.2</generator>
    
      <item>
        <title>Free adversarial training</title>
        <description>&lt;h2 id=&quot;notes-on-papers-fast-adversarial-training&quot;&gt;Notes on papers: &lt;a href=&quot;https://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf&quot;&gt;fast adversarial training&lt;/a&gt;&lt;/h2&gt;

&lt;h2 id=&quot;general-idea&quot;&gt;General idea&lt;/h2&gt;
&lt;p&gt;There were at least 2 papers at last NIPS 2019 that improved the performance of adversarial training. One of them is called ‘Adversarial training for FREE!’: https://arxiv.org/pdf/1904.12843.pdf The idea is simple and effective: reuse the gradients computed for parameters to generate adversarial examples.&lt;/p&gt;

&lt;p&gt;Main points:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The same time for standard and adversarial training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As effective as Mądry’s adversarial training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This method scales to ImageNet.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are system optimizations that can tackle main performance issues in the adversarial research.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The FREE method provides smaller accuracy than standard adversarial training on ImageNet, even though the FREE method works better on CIFAR-10 and CIFAR-100.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Description:&lt;/p&gt;

&lt;p&gt;In a typical adversarial training, in the internal maximization loop, we compute the gradients with respect to the input image many times (e.g. 7 by Mądry) for the PGD attack. In the external minimization loop, we compute the gradients once to update the parameters.&lt;/p&gt;

&lt;p&gt;The FREE method proposes to reuse the gradients computed for training (to update model parameters) and also use them for generating the attack. To increase the number of updates for PGD, they use the same batch multiple times (replay it), each time computing gradients with respect to parameters and input. To maintain the same training time, they divide the number of epochs by the number of replay steps performed on a single batch. They replay each mini batch m times before switching to the next mini batch. This strategy provides multiple adversarial updates to each training image, thus providing strong/iterative adversarial examples. Finally, when a new mini-batch is formed, the perturbation generated on the previous mini batch is used to warm-start the perturbation for the new mini-batch.&lt;/p&gt;

&lt;p&gt;The 1st reviewer of this paper noticed that: “The idea of ‘warm starting’ each new minibatch with the perturbation values from the previous minibatch is not particularly founded, and no justification or ablation study is provided to analyze the impact of this choice. What happens when no warm start is used? How much does the final attack differ from the initialization value? Is this pushing the attack towards some notion of universal perturbation?” This observation was thouroughly exploited in the the paper on fast adversarial training: https://openreview.net/pdf?id=BJx040EFvH&lt;/p&gt;
</description>
        <pubDate>Sun, 16 Feb 2020 00:00:00 +0100</pubDate>
        <link>https://adam-dziedzic.github.io//html/2020/02/16/Free-adversarial-training.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2020/02/16/Free-adversarial-training.html</guid>
        
        <category>deep learning machine learning adversarial training</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>Optasia: A Relational Platform for Efﬁcient Large-Scale Video Analytics</title>
        <description>&lt;h1 id=&quot;notes-on-a-paper&quot;&gt;Notes on a paper:&lt;/h1&gt;

&lt;h2 id=&quot;interesting-points&quot;&gt;Interesting points:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Modularizing vision pipelines in such a manner that relational query optimization can be applied.&lt;/li&gt;
  &lt;li&gt;Bringing together advances from two areas—machine vision and big data analytics systems, can lead to an efﬁcient query answering system over many cameras.&lt;/li&gt;
  &lt;li&gt;Data from survillence cameras can reach TB of storage in a week. To decrease the size of data, for example, we can suppress frames early by only emitting ones that have motion.&lt;/li&gt;
  &lt;li&gt;IBM Smart Surveillance System had a pilot deployment in Chicago (2008), developed middleware that monitors scenes, stored video in a SQL database, and provided a web interface that reported both real-time alerts and allowed for long-term pattern mining.&lt;/li&gt;
  &lt;li&gt;Optasia has the same overall goals as IBM’s S3, however, Optasia’s key contributions (improved vision modules and casting vision queries into a distributed dataﬂow system with query optimization and scale-out) substantially improve upon S3.&lt;/li&gt;
  &lt;li&gt;Typical inputs to a surveillance system are low resolution videos.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;HOG features: the histogram of oriented gradients is a feature descriptor used in computer vision and image processing for the purpose of object detection. The technique counts occurrences of gradient orientation in localized portions of an image.
&lt;!--
&lt;img style=&quot;float: center;&quot; src=&quot;static/assets/img/hog.jpg&quot;&gt;
--&gt;
&lt;img src=&quot;static/assets/img/hog.jpg&quot; alt=&quot;Example of the HOG features&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Standard feature extraction and classiﬁers sufﬁce to extract vehicle type and color from surveillance videos but they do not sufﬁce for more complex tasks such as detecting vehicle make and model.&lt;/li&gt;
  &lt;li&gt;Using key points to track objects, e.g., cars on a highway, the timestamps when they enter and exit the highway, the estimated (average) velocity.&lt;/li&gt;
  &lt;li&gt;Focus in this paper is on query answering systems over surveillance vides.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;key-contributions&quot;&gt;Key contributions:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Optasia’s key value-add is a method to modularize vision pipelines so that the result is close enough to relational algebra and then adapting existing query optimization techniques for these pipelines. In other words, Optasia’s contribution lies in translating visual queries to a format that makes relational query optimization effective.&lt;/li&gt;
  &lt;li&gt;The system consists of several vision modules, including: feature extractors (e.g. HOG), models for classification and regression (SVM, random forests), keypoint extractors (SIFT), trackers (KLT), and segmentation (MOG - Mixture of Gaussians background subraction). The modules are in C++ with the OpenCV library. OpenCV (Open Source Computer Vision) is a library of programming functions mainly aimed at real-time computer vision. Originally developed by Intel. The library is cross-platform and free for use under the open-source BSD license. OpenCV supports the deep learning frameworks TensorFlow, Torch/PyTorch and Caffe (it is used to load and show images, split images into different channels, read pixel values, etc.).&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;interesting-ideas&quot;&gt;Interesting ideas:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;What is the correct representation of videos or frames? They suggest chunks consisting of many frames, where the chunks can contain overlapping frames. Consider counting the trafﬁc volume (# of vehicles/min/lane) from a highway video. The query requires context across frames to avoid duplicate counts and hence frame-level parallelism leads to an incorrect answer. However, camera-level parallelism leads to skew (if one camera processes a busy road portion) and slow response times, because a single task has to process all the frames from a camera. By observing that the context required is bounded to the duration for which vehicles remain in the frame of reference, Optasia breaks the feed from each camera into overlapping chunks of frames. This concept of chunk-level parallelism is important to combat skew and speed-up response times. In summary: this is a chunk-level parallelism technique that allow queries to keep context for bounded time across frames. They chunk the video into overlapping groups of frames. If vehicles transit the frame-of-view in \(\delta\) frames, then chunk-n may have frames \([ns−\delta,ns+s]\). That is, the reducer processing chunk-n uses the ﬁrst \(\delta\) frames only to warm-up its internal state (e.g., assess the background for background subtraction or detect keypoints of vehicles that overlap entrance boxes); it then processes the remaining \(s\) frames. The number of the frames per chunk \(s\) and the amount of overlap \(\delta\) are conﬁguration variables. Chunk size \(s\) is calculated by comparing the beneﬁts from parallelization (smaller \(s\) implies more parallelism) with the overheads (a fraction \(\frac{\delta}{s}\) of the overall work is wasted). The amount of available resources and the need for fast query completion may also impact choice of \(s\). Observe that with chunking the available degree of parallelism is now limited only by the chunk size (\(s\)) and no longer limited by the number of cameras.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future work:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Improved video stores (compression, careful index generation).&lt;/li&gt;
  &lt;li&gt;For now, the system works in a (mini)batch mode. Applying Optasia to a distributed stream engine, especially one that scales beyond the total memory size of the cluster, is a key area of future work.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Optasia - execute vision queries on top of a distributed dataﬂow system.&lt;/li&gt;
  &lt;li&gt;The system provides a SQL-like declarative language and simpliﬁes the job of end-users and vision engineers.&lt;/li&gt;
  &lt;li&gt;It adapts a cost based query optimizer (QO) to bridge the gap between end-user queries and low-level vision modules.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 23 Aug 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//machine-learning/2018/08/23/optasia-video-analysis.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//machine-learning/2018/08/23/optasia-video-analysis.html</guid>
        
        <category>deep learning machine learning databases</category>
        
        
        <category>Machine-learning</category>
        
      </item>
    
      <item>
        <title>Spectral representations for Convolutional Neural Networks</title>
        <description>&lt;!---
In the local web browser:
   http://127.0.0.1:4000/html/2018/06/25/spectral-pooling.html
---&gt;

&lt;h1 id=&quot;notes-on-a-paper&quot;&gt;Notes on a paper:&lt;/h1&gt;
&lt;p&gt;This paper presents a very interesting idea of replacing pooling layers such as max or avg ones with spectral pooling, where the input to the pooling layer is transformed to the frequency domain via FFT, then by the heuristic that the energy of the input in the frequency domain for natural signal or images follows the inverse power law where the most information is cumulated in the low frequencies and the noise is usually represented in the high frequencies, we truncated the signal by discarding the high frequency coefficients. Finally, the truncated signal is transformed back to the time/spatial domain via the inverse FFT.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background:&lt;/h2&gt;

&lt;h3 id=&quot;unitary-transforms-matrices&quot;&gt;Unitary transforms (matrices)&lt;/h3&gt;
&lt;p&gt;A square matrix \(A=[A_1\;A_2\;\cdots\;A_n]\) (\(A_i\) for the ith column vector of \(A\)) is unitary if its inverse is equal to its conjugate transpose, i.e., \(A^{-1}=A^{*T}=A^{H}\), where the superscript \(^H\) denotes the Hermitian operation. In particular, if a unitary matrix is real \(A=A^*\), then \(A^{-1}=A^T\) and it is orthogonal. Both the column and row vectors ( \(A_i, i=1,\cdots,n\)) of a unitary or orthogonal matrix are orthogonal (perpendicular to each other) and normalized (of unit length), or orthonormal, i.e., their inner product satisfies: \((A_i, A_j) = A_i^{H}A_j\)&lt;/p&gt;

&lt;!---
$$
\begin{align*}
\left( \begin{array}{ccc}
      \phi(e_1, e_1) &amp; \cdots &amp; \phi(e_1, e_n) \\
      \vdots &amp; \ddots &amp; \vdots \\
      \phi(e_n, e_1) &amp; \cdots &amp; \phi(e_n, e_n)
    \end{array} \right)
\end{align*}
$$
---&gt;
&lt;p&gt;These \(n\) orthonormal vectors can be used as the basis vectors of the n-dimensional vector space.&lt;/p&gt;

&lt;p&gt;source: &lt;a href=&quot;http://fourier.eng.hmc.edu/e101/lectures/Image_Processing/node15.html&quot;&gt;lectures on Fourier Transorm&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;parsevals-theorem&quot;&gt;Parseval’s theorem&lt;/h3&gt;

&lt;p&gt;Fourier transform is unitary, thus the sum (or integral) of the square of a function is equal to the sum (or integral) of the square of its transform. Let \(X\) be the Discrete Fourier Transform of the sequence \(x\), then we have:
\(\sum_{i=0}^{n-1} |x_i|^2 = \sum_{F=0}^{n-1}|X_F|^2\)&lt;/p&gt;

</description>
        <pubDate>Mon, 25 Jun 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//machine-learning/2018/06/25/spectral-pooling.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//machine-learning/2018/06/25/spectral-pooling.html</guid>
        
        <category>deep learning machine learning frequency domain fft CNN convolution</category>
        
        
        <category>Machine-learning</category>
        
      </item>
    
      <item>
        <title>Parseval&apos;s identity</title>
        <description>&lt;p&gt;Geometrically, it is the Pythagorean theorem for inner-product spaces. In its special case for a single vector \(x\) it can be written as:&lt;/p&gt;

\[\sum_{n} |\prec x,e_n \succ|^2 = ||x||^2\]

&lt;p&gt;where&lt;/p&gt;

\[\prec *,* \succ\]

&lt;p&gt;represents an inner product and&lt;/p&gt;

\[||x||^{2} = \prec x, x \succ\]

&lt;p&gt;This is directly analogous to the Pythagorean theorem, which asserts that the sum of the squares of the components of a vector in an orthonormal basis is equal to the squared length of the vector. Here, we project vector \(x\) on each of the basis vectors \(e_i\).&lt;/p&gt;

&lt;p&gt;In more general form, the Parseval’s identity is:&lt;/p&gt;

\[\prec u,v \succ = \sum_i \prec u,w_i \succ \prec w_i, v \succ\]

&lt;p&gt;First, we prove that \(c_i = \prec u, w_i \succ\).
The vectors \(w_i\) for \(i=1,...,n\) are the orthonormal basis for inner-product vector space V. Then for any \(u \in V\) there exists \(c_1,c_2, ..., c_n\) such that \(u = \sum_i c_i w_i\). The coefficients \(c_i\) are called Fourier coefficients. Let’s prove that \(c_i = \prec v, w_i \succ\) using the orthogonormality (orthogonal and normal) vectors \(w_i\).&lt;/p&gt;

\[\prec v, w_i \succ = \prec \sum_{i=1}^{n} c_i w_i, w_i \succ = \sum_{i=1}^{n} c_i \prec w_i, w_i \succ = c_i\]

\[\prec w_i, w_j \succ = 1 \mbox{ if } i=j \mbox{; } 0, \mbox{ otherwise}\]

&lt;p&gt;Now, we prove the general form of Parseval’s identity:&lt;/p&gt;

\[\prec u,v \succ = \prec \sum_{i=1}^{n} c_i w_i, v \succ = \sum_{i=1}^{n} c_i \prec w_i, v \succ = \sum_{i=1}^{n} \prec v, w_i \succ \prec w_i, v \succ\]

</description>
        <pubDate>Sun, 24 Jun 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//machine-learning/2018/06/24/parsevals-identity.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//machine-learning/2018/06/24/parsevals-identity.html</guid>
        
        <category>deep learning machine learning frequency domain fft CNN convolution linear algebra LA</category>
        
        
        <category>Machine-learning</category>
        
      </item>
    
      <item>
        <title>PhD after 3rd year: a few pieces of advice</title>
        <description>&lt;h1 id=&quot;notes-on-phd&quot;&gt;Notes on phd:&lt;/h1&gt;

&lt;p&gt;I was a panelist for a discussion titled: &lt;em&gt;“A Day in the Life of a PhD Student”&lt;/em&gt;. It was addressed to students who consider pursuing PhD and currently take part in a summer research program. I really enjoy such discussions and hope that many students found some helpful suggestions.&lt;/p&gt;

&lt;p&gt;I was in a fortunate position of knowing since young age that I really wanted a PhD. Unfortunately, it wasn’t for any very well-thought-through reason: first, I liked studying and I wanted to learn as much as possible. Second, I wanted to be an inventor. This requires more knowledge and skills that hopefully I can get during my PhD.&lt;/p&gt;

&lt;p&gt;Take the advantage before PhD and get a taste of research as an undergraduate on a summer research program before you decide to commit to PhD.&lt;/p&gt;

&lt;h2 id=&quot;phd-pros&quot;&gt;PhD pros:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;your only opportunity in life to really drill deep into a topic and learn how to solve difficult problems&lt;/li&gt;
  &lt;li&gt;more freedom during PhD than in industry&lt;/li&gt;
  &lt;li&gt;more variance in your experience - you can still join a company and work in industry for next 50 years after PhD&lt;/li&gt;
  &lt;li&gt;higher starting salary after PhD&lt;/li&gt;
  &lt;li&gt;more interesting job after PhD&lt;/li&gt;
  &lt;li&gt;most exciting for those of you who are hooked at discovery: seeing something that nobody else has seen before&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;phd-cons&quot;&gt;PhD cons:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;highly competitive: working very hard (especially before paper deadlines or conferences - prepare presentation, poster, teaser talk, alone in the lab on a beautiful, sunny Saturday, mental stamina and determination to deal with the pressure, throw away 3 months of your work while somehow keeping your mental health intact)&lt;/li&gt;
  &lt;li&gt;badly paid: about 5X less than your friends at Google or Microsoft: if you are in your thirties and have to pay the mortgage load then probably this stipend is not sufficient.&lt;/li&gt;
  &lt;li&gt;family-unfriendly: not many are able to strike the right balance between family and work, only few of my PhD friends have families (usually your PhD lasts very long - 10 years or so or you simply quit it or maybe sacrifice even more important aspects of your life, e.g., the time with your children)&lt;/li&gt;
  &lt;li&gt;very hard to get a job in academia; it’s as hard to become a full professor as as it is to create a successful start-up&lt;/li&gt;
  &lt;li&gt;huge investment of time and energy - you need a few days to relax after a conference or a paper deadline (e.g. hibernate for at least 12 hours)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;what-will-you-need&quot;&gt;What will you need?&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;strong reference letters&lt;/li&gt;
  &lt;li&gt;a research publication under your belt from a summer research program is a great advantage&lt;/li&gt;
  &lt;li&gt;a few potential advisers: the adviser-student relationship is a symbiosis.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;challenges&quot;&gt;Challenges:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;you have to decide which problems are worth working on: develop a taste for a problem.&lt;/li&gt;
  &lt;li&gt;how to review papers: train your own binary classifier to cast a paper as bad or good.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;official-stages&quot;&gt;Official stages:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;coursework + master’s thesis defence (up to the first quarter of 3rd year or the end of 2nd year)&lt;/li&gt;
  &lt;li&gt;gain admission to candidacy exam (the beginning of 5th year)&lt;/li&gt;
  &lt;li&gt;write and defend a dissertation (up to the end of 6th year)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;During the first phase, students take courses, write a Master’s paper, and take an oral Master’s exam. In the second phase, students work independently to identify specific research questions of interest, master the relevant literature, and take an oral candidacy exam. And during the third phase, students do independent research and write and defend a dissertation. Throughout the process students document successful research in publications as well as in their doctoral dissertations.&lt;/p&gt;

&lt;h2 id=&quot;in-practice-there-are-slightly-different-three-stages-in-a-phd&quot;&gt;In practice there are slightly different three stages in a PhD:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;1st stage: look at a related paper’s reference section and you haven’t read most of the papers.&lt;/li&gt;
  &lt;li&gt;2nd stage: you recognize all the papers.&lt;/li&gt;
  &lt;li&gt;3rd stage: In the third stage you had lunch with all the first authors of your favorite papers.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;advisor&quot;&gt;Advisor:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;This is one of the most important choices – spend time on it and try to maintain a very good working relationship – it helps a lot.&lt;/li&gt;
  &lt;li&gt;Points you into the right direction if you’re lost.&lt;/li&gt;
  &lt;li&gt;A young on tenure-track advisor is hands-on (can give you comments such as use this tool or library or “you’re missing this on the right-hand side of the equation”), professors are usually more hand-off with comments like: “you should read more about it”.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;general-advice-on-phd&quot;&gt;General advice on PhD:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;The real value of graduate school is in education, not in degree.&lt;/li&gt;
  &lt;li&gt;Rehearse your talk before a conference until you can deliver it in your sleep.&lt;/li&gt;
  &lt;li&gt;Major deadlines always tend to fall around the same time – work hard long before them to meet all of them.&lt;/li&gt;
  &lt;li&gt;How much time you want to pour into your job? It depends on how much you want to get out of it.&lt;/li&gt;
  &lt;li&gt;Become the CEO of your own research enterprise!&lt;/li&gt;
  &lt;li&gt;You can only eat an elephant by chewing piece by piece.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;general-advice-for-summer-program&quot;&gt;General advice for summer program:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Be laser-like focused – you have only 10/12/14 weeks.&lt;/li&gt;
  &lt;li&gt;Go for every possible meeting with your advisor, post-doc or PhD student, including lunch.&lt;/li&gt;
  &lt;li&gt;Try to meet with them every every day to keep track on your progress.&lt;/li&gt;
  &lt;li&gt;Plan to write a paper after your summer program.&lt;/li&gt;
  &lt;li&gt;Go for group meetings – if possible contribute to them and, for example, volunteer to present a paper.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;read-more-thoughts-on-phd-by&quot;&gt;Read more thoughts on PhD by:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://karpathy.github.io/2016/09/07/phd/&quot;&gt;Andrej Karpathy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hamrobato.blogspot.com/2017/10/tips-for-phd-success.html&quot;&gt;Rational views&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/publication/221841574_PhD_survival_guide_Some_brief_advice_for_PhD_students&quot;&gt;Leonardo Almeida-Souza and Jonathan Baets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://phdtalk.blogspot.com/&quot;&gt;Eva Lantsoght&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://flowingdata.com/2013/04/01/a-survival-guide-to-starting-and-finishing-a-phd/&quot;&gt;Nathan Yau&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sat, 23 Jun 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//life/2018/06/23/general-advice-phd.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//life/2018/06/23/general-advice-phd.html</guid>
        
        <category>PhD life goals</category>
        
        
        <category>Life</category>
        
      </item>
    
      <item>
        <title>Price and cite data</title>
        <description>&lt;h1 id=&quot;these-are-my-notes-on-the-papers&quot;&gt;These are my notes on the papers:&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;Data Markets in the Cloud: An Opportunity for the Database Community. Magda Balazinska, Bill Howe, and Dan Suciu, VLDB 2011.&lt;/li&gt;
  &lt;li&gt;Why Data Citation is a Computational Problem? Peter Buneman, Susan Davidson, James Frew, CACM Sept. 2016.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview:&lt;/h2&gt;
&lt;p&gt;The general idea in both papers is to attach more labels to some data, or more generally, to a pair of (query, database). The labels, in this case, are prices and citations.&lt;/p&gt;

&lt;h2 id=&quot;strong-points&quot;&gt;Strong points:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;New ideas are needed how to distribute credits to authors of not only papers but also to the creators of data that are used to do research. One interesting notion is of transitivity of credit in citation, as inspired by PageRank.&lt;/li&gt;
  &lt;li&gt;Cloud based data market is happening but it seems to be still in its infancy. The valuable points are about deeply understanding how the value of data is modified during data transformations, integration, and usage, and in developing pricing/citation models/rules, supporting tools, and services to facilitate a cloud based data market as well as the citation propagation.&lt;/li&gt;
  &lt;li&gt;It seems to be a natural idea and probably most viable to set prices at the granularity of tuples (or even values within a tuple), and then extend the relational algebra based query executor to take into account prices within each operator. On the other hand, such an approach seems to be trivial, for example, when we join two tuples in a join operator, we sum the prices of the corresponding tuples to generate a price for the output tuples. It might be a bit tricky to define a simple rule for aggregation. One can imagine a simple rule for a 2 column output of a group by, where the group value is simply a sum of the grouped in rows for sum value but for a max with a group by, a price for each group would correspond with the price of the row with the max value (within a group). Then, we would have to think about the limits in a query (top 10 results), or how to cost a query that returns no results.&lt;/li&gt;
  &lt;li&gt;A price-model tuning advisor seems to be a bit far-fetched idea, though interesting. It is primarily on a higher level where we want to find a good enough pricing model for a given workload and dataset. It is similar in spirit to the Database Engine Tuning Advisor (DTA) from Microsoft SQL Server. The price-model tuning advisor would not only determine what pricing model to adopt to maximize profit but also produce income estimates.&lt;/li&gt;
  &lt;li&gt;For a consumer, it is crucial to know the estimated charges based on a given pricing model. However, with a fine-grained prices (on the level of rows or values) such estimates can be as bad as some of the query plans returned by a query optimizer.&lt;/li&gt;
  &lt;li&gt;A better and more realistic idea would be based on bid and ask price, so that the buyer and seller of data would be able to negotiate the price.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;weak-points&quot;&gt;Weak points:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;The rule-based language seems to be the first thing to explore, especially in the context of finding patterns in the data and query pairs where the same label (price or citation) can be attached. However, the rule based expert systems were replaced with much easier to create and giving better results - machine learning models. I can see the price and the citation as not a crisp labels that are deterministic but as an approximation of what we expect, thus a machine learning model to generate such metadata could be a good next step to try.&lt;/li&gt;
  &lt;li&gt;The main barrier for the pricing and citation systems can be that the data needed to generate prices or citation is simply missing, either in the database itself or in a metadata repository.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;more-notes&quot;&gt;More notes:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Construction of an argument that we need queries and data to generate a citation: Even if the query returns nothing, it may be worthy of citation, but what citation is associated with the empty set? We need at least context information; so we need both Q (query) and D (data/database).&lt;/li&gt;
  &lt;li&gt;We will need different types of citations, for instance, on Google Scholar to cite articles but also data items generated by specific researchers. The h-index and other measures will have to be adjusted to cater for the broader view of idea generation and execution.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 17 May 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/05/17/pricing-citation-data.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/05/17/pricing-citation-data.html</guid>
        
        <category>databases pricing citation data-market</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>GraphX: Graph Processing in a Distributed Dataflow Framework</title>
        <description>&lt;h1 id=&quot;these-are-my-notes-on-the-paper&quot;&gt;These are my notes on the paper:&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;GraphX&lt;/strong&gt; system tries to distill the common operations in a graph specialized system to a general data flow framework; it aims at unifying computation on tables and graphs. The data processed as collections in general data flow framework had to be harnessed/adjusted to view them as a graph (without duplication or data movement). The other motivation was to add the graph processing to the Spark eco-system, so that the whole data analytics pipeline can be executed in a single system.&lt;/p&gt;

&lt;p&gt;Modern analytics tasks (explore two views of the data - as table and also as a graph):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Pipeline 1: Raw Wikipedia data -&amp;gt; Link table (between articles) -&amp;gt; Hyperlinks -&amp;gt; Page Rank -&amp;gt; Top 20 Pages (on Wikipedia - a result stored as a materialized view in a database)&lt;/li&gt;
  &lt;li&gt;Pipeline 2: Raw Wikipedia data -&amp;gt; Discussion table (who contributed to which articles) -&amp;gt; Editor graph -&amp;gt; Community detection (run a graph algorithm) -&amp;gt; User Community (table with each user labeled with a community)&lt;/li&gt;
  &lt;li&gt;Pipeline 3: combine Page Rank and Community Detection (algorithms on graphs) -&amp;gt; Top Communities (on Wikipedia)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Separate systems for two views of the data:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Table - in dataflow systems such as Spark or Hadoop (scalable and general purpose)&lt;/li&gt;
  &lt;li&gt;Graphs - specialized graph processing systems: Pregel, GraphLab, Giraph (graph structure computation and iterative algorithms).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Points about combining different systems:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;difficult to manage and leads to creating brittle interfaces&lt;/li&gt;
  &lt;li&gt;inefficient - extensive data movement and duplication across the network and file system; limited reuse of internal data structures across stages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GraphX idea of unifying table an graph views in the library on top of a single physical computation exposed by Spark Dataflow Framework, thus enabling a single system to easily and efficiently support the entire pipeline.&lt;/p&gt;

&lt;p&gt;It is not possible to just throw away the graph specialized systems and do everything in Hadoop, e.g., the PageRank algorithm (10 iterations) on the Live-Journal Graph takes 1340 seconds on Hadoop, whereas it can be computed in 22 seconds on GraphLab. So, Hadoop ends up being 60X slower than on GraphLab. It runs in 354 seconds on Spark (16X slower than on GraphLab).&lt;/p&gt;

&lt;p&gt;Key challenge is to express and efficiently execute graph computation in a general purpose distributed dataflow framework:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;express graph representation as collections (and tables)&lt;/li&gt;
  &lt;li&gt;optimize the graph algorithms for the dataflow model&lt;/li&gt;
  &lt;li&gt;execute at big scale&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;strong-points&quot;&gt;Strong points:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;The ideas of Gonzalez, especially the scatter-gather were beautifully cast to the MapReduce jobs, where after mapping/scattering from the triplets, we get (vertex, message) pairs, that are then reduced/combined/gathered (message combiners) - it allows us to sum the message, for a particular vertex.&lt;/li&gt;
  &lt;li&gt;Using the basic GraphX operators (set of graph primitives/operations built on top of Spark) - able to implement Pregel and GraphLab in under 50 lines of code in Scala.&lt;/li&gt;
  &lt;li&gt;Introducing more scalability to the graph processing task, where scalability is for capacity and robustness (not necessarily for performance).&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;weak-points---based-on-httpwwwfrankmcsherryorggraphscalabilitycost20150115costhtml&quot;&gt;Weak points - based on: http://www.frankmcsherry.org/graph/scalability/cost/2015/01/15/COST.html&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;The data sets above are not small (billions of edges), but can run on a laptop and be much faster than the distributed systems/&lt;/li&gt;
  &lt;li&gt;GraphX uses hash tables (which can add an order of magnitude to the critical path) rather than dense arrays, to maintain its per-vertex state, to provide a layer of robustness. Fault-tolerance calls for data to be pushed from fast random access memory out to stable storage to insure against lost work in the case of a failure.&lt;/li&gt;
  &lt;li&gt;Analytics only on static graphs (what about if graph changes very often) - maybe supported within the lineage sub-system in Spark (in the future?).&lt;/li&gt;
  &lt;li&gt;Naiad (which incorporates asynchrony, not present in GraphX) was not really compared to GraphX. Naiad was found to be a way faster system, including the specialized systems.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;other-notes&quot;&gt;Other notes:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Page rank is a general algorithm for graph computation:
    &lt;ol&gt;
      &lt;li&gt;Local rank of a page can be expressed as a sum of ranks of the neighboring pages, rank of page i: R[i] = 0.15 (the random reset probability) + $\sum_{j \in Links(i)} \frac{R[j]}{OutLinks(j)}$ (a weighted sum of neighbors’ ranks).&lt;/li&gt;
      &lt;li&gt;It runs until convergence.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Vertex centric pattern of graph computation - i.e. “Think like a vertex” (Malewicz, SIGMOD 2010):&lt;/li&gt;
  &lt;li&gt;Graph-parallel pattern (Gonzalez, OSDI 2012):
    &lt;ol&gt;
      &lt;li&gt;Gather - collect information from neighboring vertices&lt;/li&gt;
      &lt;li&gt;Apply - update the vertex value&lt;/li&gt;
      &lt;li&gt;Scatter - sent the new value information to neighboring vertices&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Graph algorithms are used in machine learning and network analysis.&lt;/li&gt;
  &lt;li&gt;Machine learning with graphs: collaborative filtering (alternating least squares, stochastic gradient descent, tensor factorization), structured prediction (Loopy Belief Propagation, Max-Product Linear Program, Gibbs sampling), Semi-supervised ML (Graph SSL, CoEM).&lt;/li&gt;
  &lt;li&gt;Graph algorithm in network analysis: community detection (Triangle-counting, k-core decomoposition, k-Truss), graph analytics (PageRank, Personalized PageRank, Shortest Path, Graph coloring).&lt;/li&gt;
  &lt;li&gt;GraphX is about moving/learning from the specialized computation pattern -&amp;gt; to the specialized graph optimizations. How to create specialized optimizations to exploit the pattern.&lt;/li&gt;
  &lt;li&gt;Graph system optimizations:
    &lt;ol&gt;
      &lt;li&gt;Specialized data structures to find neighboring vertices and edges.&lt;/li&gt;
      &lt;li&gt;Vertex-cut partitioning - cut along vertices, instead of cutting along the edges, especially for power-law graphs - often seen in the real use cases.&lt;/li&gt;
      &lt;li&gt;Exploit the partitioning of the graph with remote caching/mirroring - keep copies of the vertices on remote machines, so that we can reduce the vertices for all their local adjacent neighbors.&lt;/li&gt;
      &lt;li&gt;Message combiners concept was introduced in Pregel - a commutative associative way of combining messages from remote machines. It reduces the communication via network and exploits the graph topology.&lt;/li&gt;
      &lt;li&gt;Active set tracking - track vertices that are changing for each iteration, they often are decreasing rapidly. Then, focus the computational resources on the active vertices.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;GraphX tackles the two problems:
    &lt;ol&gt;
      &lt;li&gt;Representation:
        &lt;ol&gt;
          &lt;li&gt;structure: express distributed graphs as horizontally partitioned tables&lt;/li&gt;
          &lt;li&gt;operations: how to represent a vertex program as joins (or other dataflow operators)&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;Optimizations:
        &lt;ol&gt;
          &lt;li&gt;Express/re-cast the optimizations developed for specialized graph systems in the context of more general distributed dataflow systems:
            &lt;ol&gt;
              &lt;li&gt;distributed join optimization&lt;/li&gt;
              &lt;li&gt;materialized view maintenance&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Property graph data model:
    &lt;ol&gt;
      &lt;li&gt;Vertex property
        &lt;ul&gt;
          &lt;li&gt;user profile&lt;/li&gt;
          &lt;li&gt;current PageRankValue&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Edge property
        &lt;ul&gt;
          &lt;li&gt;Weights&lt;/li&gt;
          &lt;li&gt;Timestamps&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Re-interpret the graph as tables to encode it in the dataflow framework - a normalized (3rd norm) representation:
    &lt;ol&gt;
      &lt;li&gt;Vertex Table (RDD) - Vertices stored in a horizontally partitioned table (patitioned by vertex id)&lt;/li&gt;
      &lt;li&gt;Edge Table (RDD) - Vertex cut algorithm used to partitioned the edges across the machines&lt;/li&gt;
      &lt;li&gt;Routing Table (RDD) - which edges are adjacent to a given vertex (vertex with its list of edges) - this is a type of many-to-many relationship.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Separate properties and structure: reuse structural information across multiple graphs (e.g. after a transform of the vertex properties). Many views of the same kind of data. Logical view of the graph binds the physical/tabular representation.&lt;/li&gt;
  &lt;li&gt;Triplets: logical join of the vertices and edges.&lt;/li&gt;
  &lt;li&gt;API methos:
    &lt;ul&gt;
      &lt;li&gt;reverse a graph (?)&lt;/li&gt;
      &lt;li&gt;subraph (focus on specific edges and vertices)&lt;/li&gt;
      &lt;li&gt;join - external tabular information with the graph&lt;/li&gt;
      &lt;li&gt;mrTriplets - the core operation (the map-reduce) triplets operator - to capture the gather-scatter pattern from the specialized graph processing systems.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Triplets - join vertices and edges: join the vertex table (RDD) with the edge table (RDD). For a given edge, attach to its ends - the properties of the vertices from the vertex table (RDD). We get a triplets table (Vertex1 - Edge - Vertex2). Then execute MapReduce jobs on the triple table. Map function is applied on each triplet, create a message to one of the destination vertices: Map (v1-edge-v2) -&amp;gt; (v, message). The users define a commutative-reduction operation, which is a direct analog to the message combiners.&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 15 May 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/05/15/graphx.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/05/15/graphx.html</guid>
        
        <category>osdi2014 graphs databases graphx spark</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>GeePS: Scalable deep learning on distributed GPUs with a GPU-specialized parameter server</title>
        <description>&lt;h1 id=&quot;notes-on-a-paper&quot;&gt;Notes on a paper:&lt;/h1&gt;
&lt;p&gt;GeePS is a smart data manager (data migration/movement coordinator) for a very large distributed deep learning training on GPUs. The paper addressed a real need and showed how a problem of distributed learning with GPUs can be solved.&lt;/p&gt;

&lt;h2 id=&quot;strong-points&quot;&gt;Strong points:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Three primary specializations to a parameter server to enable efficient support of parallel ML applications running on distributed GPUs:
    &lt;ol&gt;
      &lt;li&gt;Explicit use of GPU memory for the parameter cache (to increase performance): moving the parameter cache into the GPU memory enables the parameter server client library to perform the data movement steps (between CPU and GPU memory, the updates from the local GPU must still be moved to CPU memory to be sent to other machines, the updates from other machines must still be moved from CPU memory to GPU memory).
The client application parameter server executed the data movements in the background, overlapping them with GPU computing activity. In short: use GPU memory as a cache to keep actively used data and store the remaining data in CPU memory.&lt;/li&gt;
      &lt;li&gt;Batch-based parameter access methods (to increase performance) - which is very suitable for the SIMD style processing in GPU (Single Instruction on Multiple Data at once). They build indexes for faster gather-scatter operations to/from GPU registers - for the model parameters. This makes the parameter server accesses much more efficient for GPU-based training.&lt;/li&gt;
      &lt;li&gt;Parameter server management of GPU memory on behalf of the application (this expands the range of problem sizes that can be addressed). It swaps the data from the &lt;strong&gt;(not really that) limited 5 GB or more&lt;/strong&gt; GPU memory to the (much larger) CPU memory. The GPU parameter server (GeePS) manages the data buffers.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Asynchronous training has been shown to provide significantly faster convergence rates in the data-parallel CPU-based model training systems - previous CPU based parameter servers (for example, in the laster paper: Parameter server: Scaling distributed machine learning with the parameter server). However, it was shown in this paper that it does not hold with GPU-based learning. First of all, the stall time for BSP (Bulk Synchronous Parallel) is negligible so the slight training throughput speedups achieved with an asynchronous training is not sufficient to overcome the reduced training quality per image processed - thus, using BSP leads to much faster convergence. Second, the theoretical conditions for faster convergence of asynchronous training apply to convex optimization but not to the non-convex optimization, which is used in deep neural networks. There was a big problem to solve, posed by Chilimbi: “Either the GPU must constantly stall while waiting for model parameter updates or the models will likely diverge due to insufficient synchronization”. GeePS was able to reduce the stalls to the minimum (so that they are only slightly worse than for the asynchronous training).  Using GeePS, less than 8% of the GPU’s time is lost to stalls (e.g. for communication, synchronization, and data movement), as compared to 65% when using an efficient CPU-based parameter server implementation.&lt;/li&gt;
  &lt;li&gt;Interesting setup to help distributed the update of parameters in time and between many machines (this is a default configuration in GeePS): parameter data of &lt;strong&gt;each layer&lt;/strong&gt; is stored in a distinct table, the parameter updates of a layer can be sent to the server and propagated to other workers (in the background) before the computation of other layers finish. This is about decoupling the parameter data of different layers. A less efficient setup would be to store all an application’s parameter data in a single table (the parameter updates of all layers would be sent to the server together as a whole batch).&lt;/li&gt;
  &lt;li&gt;The results are impressive, as expected after harnessing many GPUs. For example, GeePS achieves a higher training throughput with just four GPU machines than a state-of-the-art CPU-only system achieves with 108 machines.&lt;/li&gt;
  &lt;li&gt;Resource division: this is a good division of resources: the CPU part of a node is for the parameter server while the GPU part of the node is used to do the training of a machine learning model. The GPU memory is the primary memory for the training data (mini-batches) and currently used parameters, the CPU memory is used as the parameter server memory but also as a bigger storage and swapping space for the data that does not fit into the GPU memory.&lt;/li&gt;
  &lt;li&gt;Avoiding data copying by exchanging pointers to GPU buffers. “The parameter server uses pre-allocated GPU buffers to pass data to an application, rather than copying the parameter data to application provided buffers. When an application wants to update parameter values, it also does so in GPU allocated buffers. The application can also store local non-parameter data (e.g intermediate states) in the parameter server.”&lt;/li&gt;
  &lt;li&gt;Very nice pinning of “pages” that are needed in both GPU and CPU.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;week-points&quot;&gt;Week points:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;They do not try to mitigate or even address in any way the problem of stragglers.&lt;/li&gt;
  &lt;li&gt;The main point is that the training of deep nets is effective on GPUs when model, intermediate state and input mini-batch fit into the GPU memory. All the models that they used in their experiments fit all the required data into the GPU 5 GB memory. When the GPU memory limits the scale, the common solution is to use smaller mini-batch sizes to let everything fit in GPU memory. However, there are two problems with small batches: 1) longer training time - the more mini-batches the more time spent on reading and updating the parameter data locally (with larger mini-batches it is amortized over time) 2) the GPU computation is more efficient with a larger mini-batch size.&lt;/li&gt;
  &lt;li&gt;They don’t address the problem of resilience. The usage of write-back cache implies that the parameters are kept only in cache and if one of the nodes crushes then we can loose part of the model parameters - it is not clear if the model can still converge but with only 4 param servers, it might be rather impossible. Thus, the system focuses primarily on performance without addressing any issues of lack of robust execution and node failures.&lt;/li&gt;
  &lt;li&gt;It can handle models as large as 20 GB while currently a 13 GB or more in a GPU is a common size. Thus, the system will be obsolete very soon if not expanded to cater to larger models (but is there such a need?).&lt;/li&gt;
  &lt;li&gt;There is a missing arrow in the graphs between the “Parameter server shard 0” and “Staging memory for parameter cache”. If the worker and parameter server processes are collocated, the exchange of updated parameters does not have to go through the network.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;bulk synchronous parallel (BSP) involves three main steps: 1) local computation on many machines 2) communication between the machine 3) reaching a barrier and global step to the next task. In other words, the system includes: 1) components capable of processing and/or local memory transactions (i.e., processors) 2) a network that routes messages between pairs of such components, and 3) a hardware facility that allows for the synchronisation of all or a subset of components.&lt;/li&gt;
  &lt;li&gt;DeepImage - a specialized system with distributed GPUs connected via RDMA without any involvement of CPUs.&lt;/li&gt;
  &lt;li&gt;Krizhevsky - did he come up with the idea of dividing the model into many GPUs by placing each layer on a separate GPU? but this would require the expensive exchange of the intermediate data/state for the forward and backward passes.&lt;/li&gt;
  &lt;li&gt;Double buffering is used by default to maximize overlapping of data movement with computation. I observed a very bad behavior in many DBMSs for data loading/migration/export because of explicit lack of simultaneous computation (parsing &amp;amp; deserialization) and data movement (fetching data from disk for loading or moving the parsed/loaded data from the memory to a disk storage). The data movement and computation are not interleaved in many cases and we have to wait with computation until, for example, the local buffers are dumped to disk.&lt;/li&gt;
  &lt;li&gt;Most layers have little or no parameter data, and most of the memory is consumed by the intermediate states for neuron activations and error terms. (section 5.3 - artificially shrinking available GPU memory).&lt;/li&gt;
  &lt;li&gt;Gather - fill the SIMD register with values from different memory locations. Similarly, scatter - write the values in the output SIMD register to different memory locations. Gather-scatter is a type of memory addressing that often arises when addressing vectors in sparse linear algebra operations. It is the vector-equivalent of register indirect addressing, with gather involving indexed reads and scatter indexed writes. Vector processors (and some SIMD units in CPUs) have hardware support for gather-scatter operations, providing instructions such as Load Vector Indexed for gather and Store Vector Indexed for scatter.&lt;/li&gt;
  &lt;li&gt;GeePS is a parameter server supporting data-parallel model training. In data parallel training, the input data is partitioned among workers on different machines, that collectively update shared model parameters. These parameters themselves may be sharded across machines. “This avoids the excessive communication delays that would arise in model-parallel approaches, in which the model parameters are partitioned among the workers on different machines.”&lt;/li&gt;
  &lt;li&gt;In the basic parameter server architecture, all state shared among application workers (i.e. the model parameters being learned) is kept in distributed shared memory implemented as a specialized key-value stare called a “parameter server”. An ML application’s workers process their assigned input data and use simple Read and Update methods to fetch or apply a delta to parameter values, leaving the communication and consistency issues to the parameter server.&lt;/li&gt;
  &lt;li&gt;Client-side caches are also used to serve most operations locally. Many systems include a Clock method to identify a point when a worker’s cached updates should be pushed back to the shared key-value store and its local cache state should be refreshed.&lt;/li&gt;
  &lt;li&gt;While logically the parameter server is separate to the worker machines, in practice the server-side parameter server state is commonly sharded across the same machines as the worker state. This is especially important for GPU-based ML execution, since the CPU cores on the worker machines are otherwise underused.&lt;/li&gt;
  &lt;li&gt;When we train neural networks (forward and backward passes): each time only data of two layers are used.&lt;/li&gt;
  &lt;li&gt;Interface to GeePS managed buffer”
    &lt;ol&gt;
      &lt;li&gt;Read - Buffer “allocated” by  GeePS, data copied to buffer.&lt;/li&gt;
      &lt;li&gt;PostRead - Buffer reclaimed&lt;/li&gt;
      &lt;li&gt;PreUpdate - Buffer “allocated” by GeePS&lt;/li&gt;
      &lt;li&gt;Update - Updates applied to data. Buffer reclaimed.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;CPU PS (Parameter Server) has much overhead of transferring data between GPU/CPU memory in the foreground.&lt;/li&gt;
  &lt;li&gt;Implementation: GeePS is a C++ library. The ML application worker often runs in a single CPU thread that launches NVIDIA library calls or customized CUDA kernels to perform computations on GPUs, and it calls GeePS functions to access and release GeePS-managed data. The parameter data is sharded across all instances, and cached locally with periodic refresh (e.g. every clock for BSP). GeePS supports BSP, asynchrony, and the Staleness Synchronous Parallel (SSP) model.&lt;/li&gt;
  &lt;li&gt;Moreover, the GeePS can serve as a cache &lt;strong&gt;manager&lt;/strong&gt; in general. The local non-parameter data (the intermediate state - data exchanged between the layers in the neural network) that probably do not fit into the GPU memory can be spilled to the CPU memory and managed the GeePS.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;parameter-servers-on-cpus--gpus&quot;&gt;Parameter servers on CPUs &amp;amp; GPUs:&lt;/h3&gt;

&lt;h4 id=&quot;main-differences&quot;&gt;Main differences:&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;CPU PS - focused on machine learning in general, on the other hand, Gee PS - focused on deep learning.&lt;/li&gt;
  &lt;li&gt;deep learning is non-convex - synchrony is required&lt;/li&gt;
  &lt;li&gt;the CPU PS addresses convex problems - may work with asynchronous updates&lt;/li&gt;
  &lt;li&gt;mini-batch can cater to the big data - the data is naturally partitioned&lt;/li&gt;
  &lt;li&gt;the bottleneck of data movement between GPU memory and CPU memory&lt;/li&gt;
  &lt;li&gt;limited GPU memory - the huge problem that they focused on here&lt;/li&gt;
  &lt;li&gt;cache also the intermediate data (the data exchanged between neural network layers)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;similarities&quot;&gt;Similarities:&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;the models are big - billions of parameters&lt;/li&gt;
  &lt;li&gt;gobs of training data&lt;/li&gt;
  &lt;li&gt;distribution (scale out)&lt;/li&gt;
  &lt;li&gt;iterative computation (until we reach a convergence)&lt;/li&gt;
  &lt;li&gt;traditional algorithms&lt;/li&gt;
  &lt;li&gt;traditional algorithms/systems are single node&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;gpus&quot;&gt;GPUs:&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;stalling for data&lt;/li&gt;
  &lt;li&gt;libraries for fast linear algebra&lt;/li&gt;
  &lt;li&gt;underutilized cores&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;virtual-mode&quot;&gt;Virtual-mode:&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;hypothetically - what-if mode for the query optimizer&lt;/li&gt;
  &lt;li&gt;build the indexes&lt;/li&gt;
  &lt;li&gt;run the transactions without any locks&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;more-notes&quot;&gt;More notes:&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;color-coded graphs&lt;/li&gt;
  &lt;li&gt;write-ahead logging is exploiting the idea of simultaneous computation and data transfer&lt;/li&gt;
  &lt;li&gt;all the GPU memory goes to intermediate state (the data exchanged between layers)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other-sources&quot;&gt;Other sources:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Morning paper: https://blog.acolyer.org/2016/04/27/geeps-scalable-deep-learning-on-distributed-gpus-with-a-gpu-specialized-parameter-server/&lt;/li&gt;
  &lt;li&gt;Slides: https://www.cs.cmu.edu/~hzhang2/projects/GeePS/slides.pdf&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 10 May 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/05/10/GeePS.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/05/10/GeePS.html</guid>
        
        <category>machine-learning Euro 2014 databases optimization param-server parameter-server parameter server</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>Parameter server: Scaling distributed machine learning with the parameter server</title>
        <description>&lt;h1 id=&quot;these-are-my-notes-on-the-paper&quot;&gt;These are my notes on the paper:&lt;/h1&gt;

&lt;p&gt;This paper is about one of the two main components of machine learning, namely optimization (the architecture and model selection being the other major component).&lt;/p&gt;

&lt;p&gt;The asynchronous parallel optimization recently received many successes and broad attention in machine learning and optimization. It is mainly due to that the asynchronous parallelism largely reduces the system overhead comparing to the synchronous parallelism. The key idea of the asynchronous parallelism is to allow all workers work independently and have no need of synchronization or coordination. The asynchronous parallelism has been successfully applied to speedup many state-of-the-art optimization algorithms including stochastic gradient descent (&lt;a href=&quot;http://papers.nips.cc/paper/5751-asynchronous-parallel-stochastic-gradient-for-nonconvex-optimization.pdf&quot;&gt;source&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;strong-points&quot;&gt;Strong points:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;A core goal of the parameter server framework is to capture the benefits of GraphLab’s asynchrony (to schedule communication using a graph abstraction) without its structural limitation that impede scalability and elasticity (namely, the lack of global variable and state synchronization/sharing as an efficient first-class primitive).&lt;/li&gt;
  &lt;li&gt;Machine Learning specialized optimizations: message compression, replication, and variable consistency models expressed via dependency graphs. They achieve synergy by picking the right systems techniques, adapting them to the machine learning algorithms, and modifying the machine learning algorithms to be more systems friendly. The trade-off between the system efficiency and algorithm convergence rate depends on: algorithm’s sensitivity to data inconsistency, feature correlation in training data, capacity difference of hardware components, and many other factors. Algorithm designer can define consistency models via the “bounded delay” method. Moreover, the scheduler may increase of decrease the maximal delay according to the runtime progress to balance system efficiency and convergence of the underlying optimization algorithm.&lt;/li&gt;
  &lt;li&gt;Parameter server is able to cover orders of magnitude more data on orders of magnitude more processors than any other published system. It simplifies the development of distributed machine learning applications. It enabled LDA (Latent Dirichlet Allocation) models with 10s of billions of parameters to be inferred from billions of documents, using up to thousands of machines&lt;/li&gt;
  &lt;li&gt;Natural but also clear division of BOTH: the input data and the parameters (in the form of key, value pairs) between many worker nodes and parameter server nodes, respectively. This approach allows them to scale the computing resources, the data used for training machine learning models and the models themselves (with billions of parameters). It can also work by communicating only a range of keys/parameters at a time.&lt;/li&gt;
  &lt;li&gt;Both key/parameter caching and data compressing system-level optimization are generalized to user-defined filters.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;weak-points&quot;&gt;Weak points:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Do not offer higher-level general-purpose building blocks such as model partitioning strategies, scheduling, managed communication that
are key to simplifying the adoption of a wide range of ML methods.  In general, systems supporting distributed ML manifest a unique trade-off between efficiency, correctness, programmability, and generality.&lt;/li&gt;
  &lt;li&gt;They used the traditional techniques from distributed systems with some small improvements (vectors clocks compressed taking into account that not many vector clocks diverge), snappy compression applied on messages - to compress zeros as user-defined filters may zero out large fraction of the parameters.&lt;/li&gt;
  &lt;li&gt;Many trade-offs: accuracy of the machine learning model and the model for consistency. They don’t provide any graphs on how their methods hurt the machine learning models accuracy!&lt;/li&gt;
  &lt;li&gt;Bounded delays were discussed in many previous paper (also by Alexander J. Smola). There is a very similar paper to this one (repeating even the figures) in the NIPS conference.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 08 May 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/05/08/parameter-server.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/05/08/parameter-server.html</guid>
        
        <category>machine-learning OSDI 2014 databases optimization param-server parameter-server parameter server</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>Notes on a paper: Spark Streaming</title>
        <description>&lt;h1 id=&quot;these-are-my-notes-about-the-paper-discretized-streams-fault-tolerant-streaming-computation-at-scale&quot;&gt;These are my notes about the paper &lt;a href=&quot;https://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf&quot;&gt;“Discretized Streams: Fault-Tolerant Streaming Computation at Scale”&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;strong-points&quot;&gt;Strong points:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Change of the paradigm - stream processing is no longer full of state maintenance but closer to the batch processing with simpler fault tolerance handling, fast recovery and mitigating the effect of stragglers.&lt;/li&gt;
  &lt;li&gt;The handling of fault-tolerance is better - it uses PARALLELISM 1) within a batch 2) across time intervals. The recovery is faster than for the upstream backup, especially because of the nice property that the larger the cluster, the lower the recovery time. The parallel recovery also applies to stragglers - it mitigates them by 1) detecting slow tasks (e.g., 2X slower than the other tasks on average) 2) speculatively launching more copies of the tasks in parallel on other machines. The net effect is that the speculative launch of redundant tasks masks the impact of slow nodes on the holistic progress of the system. The previous methods of recovery in streaming systems were much more complicated, for example, there were special protocols needed to synchronize the master nodes with the “slave” nodes - e.g. Borealis’ DPC). The checkpointing is done asynchronously to bound the recovery time.&lt;/li&gt;
  &lt;li&gt;The whole idea behind the streaming and batch processing in Spark is based on a beautiful and neat application of functional programming paradigms to data processing. There is a unification of batch and streaming - a single programming and execution model for running streaming, batch and interactive jobs.&lt;/li&gt;
  &lt;li&gt;User friendly - for in-memory and on-line debugging. Users can fire a Spark console and query the streams on the fly / ad-hoc. It was very easy to implement the Markov Chain Monte Carlo simulation for traffic estimation from GPS data.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;weak-points&quot;&gt;Weak points:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;No support for watermarks - a feature for dropping overly old data when sufficient time has passed. The first approximation of the feature is the implicit boundary between the small batches.&lt;/li&gt;
  &lt;li&gt;The main problem of the system is its long latency - a fixed minimum latency due to batching data (though some continuous operator systems, such as Borealis, add delays to ensure determinism). The obvious way to mitigate it is to decrease the window size and avoid firing a new process/task for each batch - but simply keep a pool of tasks that process the batches in a continuous way. It was addressed in the latest post from Databricks on streaming - millisecond low-latency of streaming - continuous mode that was built to process each transaction in the pipeline within 10-20 ms (as it is the case for the detection of fraudulent transactions). The main new contribution is the usage of continuous processing that uses long running tasks to continuously process events. Moreover, the progress is checkpointed by an adaptation of Chandy-Lamport algorithm. The fine-grained lineage graph helps to save on copying the data - on the other hand, the asynchronous checkpoints are used to prevent long lineages (to recompute lost partitions).&lt;/li&gt;
  &lt;li&gt;The experimental part is a bit straw-man comparison: it is possible to reduce the overhead of number of nodes for the upstream backup from 2X to 1.1X (for example it is possible to only send the write-log, or even a simple separation of data writes and log writes gives only about 15% overhead of the processing).&lt;/li&gt;
  &lt;li&gt;The notion of time is a bit distorted and they do not pay enough attention - through the whole paper it is assumed that the input data is time-stamped at the receiving node.&lt;/li&gt;
  &lt;li&gt;Not that many modification is Spark were needed to propose the streaming part - e.g. reduce the latency of launching the jobs (as a big initial overhead) from seconds to milliseconds.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;The main requirement - large scale streaming systems must handle faults and stragglers.&lt;/li&gt;
  &lt;li&gt;Naiad - combines graph, streaming and batch processing without sacrificing latency but a full cluster rollback is required on recovery.&lt;/li&gt;
  &lt;li&gt;Great slides but also some complaints: https://www.slideshare.net/ptgoetz/apache-storm-vs-spark-streaming&lt;/li&gt;
  &lt;li&gt;Second latency acceptable to many applications (maybe exlucding the case of high-frequency algorithmic trading).&lt;/li&gt;
  &lt;li&gt;Spark 2.3 - they go to the continuous processing - they support users’ timestamps - the initial problems were solved eventually.&lt;/li&gt;
  &lt;li&gt;No-batch - get a single answer faster, but hurts the throughput.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 06 May 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/05/06/Spark-Streaming.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/05/06/Spark-Streaming.html</guid>
        
        <category>streaming databases SOSP 2013 spark mike</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>Notes on a paper: Learned indexes</title>
        <description>&lt;h1 id=&quot;the-case-for-learned-index-structures&quot;&gt;The Case for Learned Index Structures&lt;/h1&gt;

&lt;p&gt;I think that if the learned indexed work as presented in the experiments, then they (i.e. Tim Kraska and the Google’s team) found a new approach how to leverage Machine Learning (specifically Neural Networks in the future of more integrated CPU-GUP-TPU hardware) to improve the “old” database architecture in a bit different way. On the high level, they treat a B+tree as a model or simply a function from a search key (or range) to an output position. They clearly motivate that only with the future performance improvement of GPUs/TPUs (1000x in 2025) we can achieve much better performance with Neural Networks than with B+trees (that work on CPUs) - section 2.1. Taking into consideration my work from the last internship at MSR (Microsoft Research), they definitely should compare with column store indexes (especially sorted ones for the read-only workload – they focus primarily on the analytical workloads anyway). Further on, they do provide many details, e.g. compare B+trees and Neural Networks comprehensively in the experimental part, and even go into the implementation details of the dense/sparse hash maps (Google has its own internal highly-optimized hash-maps).&lt;/p&gt;

&lt;p&gt;Regarding auto indexing, it would change/extend the arena quite a bit. Of course, we still have to decide on which attributes we should build the indexes on, but the space of possible indexes would be much broader (many more possible models, or we could just map B+trees, hash maps etc. to specific per-arranged models) and the parameters for a given index could be found automatically: “Furthermore, auto-tuning techniques could be used to further reduce the time it takes to find the best index configuration” (but by this auto-tuning they mean selecting parameters for a single model/index).&lt;/p&gt;

&lt;p&gt;So, the performance tuning of the database with learned indexes would be much harder – not only selecting attributes for indexes but also tuning the parameters for indexes themselves. However, this approach to indexing strengthens the motivation for auto-admin tools and poses new interesting challenges.&lt;/p&gt;

&lt;p&gt;I think that it also depends on the use case. I was an intern at Google and read a few papers where they argue that in many cases they have to analyze humongous data sets (that usually are read or append only). The possibility of decreasing the size of the indexes significantly and no need for updates seems to be like the place the learned indexes can excel at.&lt;/p&gt;

&lt;p&gt;One area that should be still explored is how to cater for workloads with many updates/inserts/deletes for the learned indexes. Interestingly enough, if, let’s say, we have the append-only workload and the data distribution does not change then we don’t really have to modify the index.&lt;/p&gt;

&lt;h2 id=&quot;strong-points&quot;&gt;Strong points:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;There are already databases that are only-GPU based, so such indexes would be a perfect match.&lt;/li&gt;
  &lt;li&gt;When we zoom in to individual record on the CDF curve (Cumulative Distribution Function), we can see many discontinuities which are hard to learn - too much entropy. The idea with the last mile access supported with B+trees seems to be viable - even with a better approach to explore the benefits of both B+trees and machine learning models within the same index. Then, we could adjust the percentage of usage of both structures while building the index to find the the best performing combination,  usually - with B+tree at the bottom of the index, and the machine learning models (simple linear models or simple neural nets) on the top of the index.&lt;/li&gt;
  &lt;li&gt;They use interesting tricks, for example, let’s extract the model, i.e. all the weights, to a bare structure in C++ that eludes all the overheads by shedding the Python interface, additional function calls or batch processing used for large scale machine learning.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;weak-points&quot;&gt;Weak points:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;A costly procedure to give the error upper bounds (min/max error) where the key should be searched within.&lt;/li&gt;
  &lt;li&gt;If a key does not exist in the index, the index can point to the range which is outside the min-max range, maybe we would end up with traversing the whole dataset to make sure that a key does not exist.&lt;/li&gt;
  &lt;li&gt;They don’t explore the interesting scope of the relation between precision required and complexity of the model, subsequently between the complexity of the model and the search time. These are the crucial dimensions to judge the performance of the new proposed indexes.&lt;/li&gt;
  &lt;li&gt;If we want to use GPUs/TPUs we have to deal with the long transfer time between CPU and GPU which is in 1000 of cycles, whereas scanning a leaf node from a B+tree, even with a cache-miss takes only 150 cycles (50 cycles for the scan and additional 100 cycles incurred by the cache-miss).&lt;/li&gt;
  &lt;li&gt;Clearly, the vanilla models presented in section 2.3 are extremely bad and much slower than B+trees: “The search time to find the data with the help of the prediction almost had no benefit over a full search over the data”.&lt;/li&gt;
  &lt;li&gt;B+trees are cache efficient (keeping only the top of the trees in cache), whereas the neural nets require all the weights to compute the prediction.&lt;/li&gt;
  &lt;li&gt;There is a dichotomy of errors: in the machine learning models we optimize for the average error while for the learned indexes we are optimizing for the min-max errors (the min-max range of positions that we have to check to answer the question if the searched key is present, and if so where, or is absent in the index).&lt;/li&gt;
  &lt;li&gt;The biggest challenge is the last-mile accuracy.&lt;/li&gt;
  &lt;li&gt;It is not crystal clear how they train the staged models - they do give the loss functions but they do not do the full end-to-end training, I suppose for a model lower in the stage, only the search keys that go through the model in the current learning stage, contribute to the loss - but these keys taken into account for the less in a staged model might be very different through the learning process, i.e. fluctuate a lot, thus it is not explained when we stop training and if the loss really decreases and model converges.&lt;/li&gt;
  &lt;li&gt;New leaf-node search strategies - does not seem to be like a great gain in performance.&lt;/li&gt;
  &lt;li&gt;Why don’t we use word embeddings for the indexing of strings?&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;other-notes&quot;&gt;Other notes:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Regarding the Clipper system - this paper states that you should never use TensorFlow for inference.&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Fri, 04 May 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/05/04/learned-indexes.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/05/04/learned-indexes.html</guid>
        
        <category>machine-learning SIGMOD 2018 learned-indexes</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>Clipper</title>
        <description>&lt;h1 id=&quot;clipper&quot;&gt;Clipper:&lt;/h1&gt;

&lt;h1 id=&quot;these-are-my-notes-on-the-paper&quot;&gt;These are my notes on the paper:&lt;/h1&gt;

&lt;p&gt;In general, Clipper is similar to BigDAWG. However, instead of serving answers from many databases, it serves predictions from many machine learning models for the task of inference (which is analogous to the query execution in databases). Both systems cater to the users who care about the best performance, in terms of latency and throughput, and have enough resources in terms of hardware and human expertise to deploy many machine learning models in different frameworks, or deploy many database systems and manage them. The manageability cost in both cases is not negligible. The advantage of Clipper lies in its main goal and task - namely, the machine learning inference exposes a much simpler interface, e.g., List&amp;lt;List&lt;Y&gt;&amp;gt; predict(List&lt;X&gt; input); whereas the BigDAWG system was designed to accept queries in different forms - spanning from typical SQL (in different dialects) to even array-oriented language. Probably, BigDAWG should expose a simpler interface - e.g. a functional-like one, and then leverage connectors to the database for such language binding (e.g. Scala) - which would simplify the whole translation layer (from a user query - routing it to the destination database engine). Each model container resides in a separate Docker container - the same technique was used in BigDAWG, where separate Database Systems reside in independent Docker containers.&lt;/X&gt;&lt;/Y&gt;&lt;/p&gt;

&lt;h2 id=&quot;strong-points&quot;&gt;Strong points:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Three crucial properties of the predicting system: low latencies, high throughput, improved accuracy. A good system design with batching, caching and other standard techniques harnessed for the much faster, better (more accurate) predictions. For instance, maintaining a prediction cache, Clipper can serve frequent queries without evaluating the model This reduces latency and system load by eliminating the additional cost of model evaluation. This is vital, since the prediction/inference is bottlenecked on the computation / CPU time - thus the caching for the past predictions gives the biggest bang for the buck. Start-up cost can be reduced by pre-warming the model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is claimed that batching can amortize the RPC calls. From the experimental part, we know that this is only a negligible cost. The real cost is the prediction/inference itself on a GPU. The batching mechanism can increase throughput, indeed, significantly because this is how the machine learning algorithms operate internally - on vectors and matrices (use BLAS libraries) - the input should be given as an array of examples - treated internally as a matrix or tensor. So, batching closely match the workload assumptions made by machine learning frameworks. Another overhead mitigated by batching is the cost of copying inputs to GPU memory. The AIMD scheme was used to tune the size of the batch - additively increase the batch size by a fixed amount until the latency to process the batch exceeds the objective, then multiplicatively (by a small percentage) start decreasing the batch size. Batch delay policy - helped only in case of Scikit-Learn - 2ms batch delay provided 3.3X improvement in throughput and the latency remained in the window of 10-20ms objective.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One difference between model ensemble and Clipper is that the ensemble method is focused on improving only the accuracy, whereas Clipper can also boost the performance of the whole system by lowering the latencies. Moreover, it provides mechanisms to easily navigate the trade-offs between accuracy and computation cost on a per-application basis.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I had the idea of extending the streaming system with certainty bounds on the results for the processing windows (e.g. the sliding window) - Clipper gives confidence levels of the predictions - thanks to using many models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The accuracy of a deployed model can silently degrade over time. Clipper’s online selection policies can automatically detect these failures using feedback and compensate by switching to another model (Exp3) or down-weighting the failing model (Exp4).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Horizontal scaling achieved without sacrificing the latency or accuracy (in terms of parallel systems).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;weak-points&quot;&gt;Weak points:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;ML model in Clipper is treated as a black-box, for instance, we cannot optimize the execution/inference of the model. On the other hand, TensorFlow serving is able to leverage GPU acceleration and compilation techniques to speedup the inference. Furthermore, TensorFlow Serving tightly couples the model and serving components in the same process.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Even more detrimental effect of the non-transparent layered design is the inability to update the model - the feedback loop operates on the level of choosing already trained models, but the model could be updated and made better based on their predictions and the feedback received from the end application (to re-train the model). Thus, Clipper does not close the loop of: learn -&amp;gt; deploy -&amp;gt; explore -&amp;gt; log -&amp;gt; learn. The feedback (log) is not provided to re-learn the model. If all models are out-of-date - then the accuracy of the predictions returned by Clipper will be miserly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some figures are awry - for example, Figure 4. all numbers (labels) are the same in the top and bottom parts of the figure - for denoting the values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adaptive model selection occurs above the cache in Clipper, thus changes in predictions due to model selection do not invalidate cache entries. The layers could have been a bit less transparent and we could have something like a down-stream call - to clear the cache from the results of an old model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Clipper studies multiple optimization techniques to improve the throughput (via batch size) and reduce latency (via caching). It also proposes to do model selection for ensemble modeling using multi-armed bandit algorithms. We could do better by providing both training and inference services together. Clipper optimizes the throughput, latency and accuracy separately (in a step/stage approach), we could model them together to find the optimal model selection and batch size.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Paper does not give a real-world clear application experiments that would show that Clipper does indeed improve the end-to-end inference time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Techniques used on the technical level are rather regular. The slow inference cannot be solved only on the system level, we need a collaboration with the hardware community to provide specialized accelerators. Interestingly enough, TPUs take a lower precision input to accelerate the computation. How to compress the computational footprint for deep networks and the storage footprint for input data?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;other-notes&quot;&gt;Other notes&lt;/h2&gt;
&lt;p&gt;It would be great to explore how difficult it is to extend TensorFlow Serving tool - we could train many models with TensorFlow and add the feature of serving many models.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model gets stale because things are changing in the outside environment.&lt;/li&gt;
  &lt;li&gt;Folklore - different groups doing these stages: data scientists - build / train the model, devops - deploy the model and work at scale; bridge the gap from the batch training to the deployment at scale;&lt;/li&gt;
  &lt;li&gt;stream vs. batch processing have different requirements&lt;/li&gt;
  &lt;li&gt;TensorFlow is a moving target - it changes all the time&lt;/li&gt;
  &lt;li&gt;modular design - does not introduce undue overheads&lt;/li&gt;
  &lt;li&gt;In TensorFlow, the queues are pushed into the TensorFlow framework&lt;/li&gt;
  &lt;li&gt;Clipper - scheduling decisions made twice&lt;/li&gt;
  &lt;li&gt;wrappers - Jim Gray’s paper - transparency in its place&lt;/li&gt;
  &lt;li&gt;flexibility of the ensembles - abstractions added in Clipper do not add that much overhead&lt;/li&gt;
  &lt;li&gt;answer queries in the live-way&lt;/li&gt;
  &lt;li&gt;context - AMP lab - rolling alone - by hand - everybody was building their own databases before&lt;/li&gt;
  &lt;li&gt;pull out / define new category of software - common software&lt;/li&gt;
  &lt;li&gt;mem-cache - distributed store - sort of intermediary caching systems - wrappers - REST based API&lt;/li&gt;
  &lt;li&gt;RDS - model selection / RSS - model abstraction&lt;/li&gt;
  &lt;li&gt;protocol conversion/ glue&lt;/li&gt;
  &lt;li&gt;applications/users on the very top&lt;/li&gt;
  &lt;li&gt;mediator/wrapper architecture&lt;/li&gt;
  &lt;li&gt;how to abstract a set of different systems or algorithms&lt;/li&gt;
  &lt;li&gt;how do we deal with machine learning models&lt;/li&gt;
  &lt;li&gt;how to get a feedback from the system&lt;/li&gt;
  &lt;li&gt;what’s new here? -&lt;/li&gt;
  &lt;li&gt;call for papers - SIGMOD 2019 - database community - debate&lt;/li&gt;
  &lt;li&gt;questions - what constitutes a new systems work - no innovation in the individual boxes - model selection (multi-arm bandit algorithm), adaptive batch size - additive increase, multiplicative decrease - AIMD.&lt;/li&gt;
  &lt;li&gt;systems work - defining abstractions - TensorFlow serving,&lt;/li&gt;
  &lt;li&gt;model servers did not exist&lt;/li&gt;
  &lt;li&gt;various external interfaces - how to think about the abstractions inside&lt;/li&gt;
  &lt;li&gt;serve predictions with low latency&lt;/li&gt;
  &lt;li&gt;provide accurate answer&lt;/li&gt;
  &lt;li&gt;model selection - comes from the machine learning literature&lt;/li&gt;
  &lt;li&gt;model abstraction - comes from the systems works + scheduling&lt;/li&gt;
  &lt;li&gt;systems paper biggest advantage - your system can make something much easier than it was in the past&lt;/li&gt;
  &lt;li&gt;not enough innovation&lt;/li&gt;
  &lt;li&gt;Spark - fault tolerance - RDD is all about it&lt;/li&gt;
  &lt;li&gt;what is the emphasis of the paper?&lt;/li&gt;
  &lt;li&gt;VELOX - many specialized models sit on top of a single/bottom model&lt;/li&gt;
  &lt;li&gt;select-combine-observe loop - they don’t go into much detail how the things are personalized&lt;/li&gt;
  &lt;li&gt;2 modes of model selection - pick a single best model - the most accurate answer - they went to the multi-arm bandit - bunch of system that give you answer - which one is giving you the best answer - you explore - try different arms - the slot machine - with the best pay-out - if you don’t try the other ones - you are stuck in local optimum&lt;/li&gt;
  &lt;li&gt;Is there a new class of systems?&lt;/li&gt;
  &lt;li&gt;Scale from 4 models to 4000? Can we build it with 1000 models?&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 01 May 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/05/01/clipper.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/05/01/clipper.html</guid>
        
        <category>machine-learning NSDI 2017</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>Database Tuning Advisor for Microsoft SQL Server 2005</title>
        <description>&lt;h1 id=&quot;database-tuning-advisor-for-microsoft-sql-server-2005&quot;&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/VLDB04.pdf&quot;&gt;Database Tuning Advisor for Microsoft SQL Server 2005&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;The paper is an overview of the DTA (Database Tuning Advisor) which was release in 2005. The tool is able to recommend indexes, materialized views and partitioning (usually ranged and aligned - the same partitions are present on tables and indexes). There were several automated physical design tools developed by commercial database vendors, however, the DTA tool seemed to be the most advanced one. For example, it offered fully integrated recommendation, where, for instance, indexes and materialized views were considered simultaneously and not in a one-by-one fashion or so called in the staging manner. These tools consider not only the single objective of improving a general performance of a given workload, but also take into account scalability (the tuning time should be reasonable and do not impose too much overhead on a production server), manageability (the usage of horizontal range partitioning to ensure easy backup/restore, to add new data or remove old data), and functionality (the features provided by the DTA - e.g., scriptability - an XML schema for input/output that enhances scriptability and cusomizability of the tool; and efficient tuning in production/test server scenarios).&lt;/p&gt;

&lt;p&gt;Back then, one of the main concern was the scalability. This was addressed in DTA as well by enabling it to scale to large databases and workloads via:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;workload compression (extract query signatures which denote the query templates; consider a single query per template; the workload to be optimized consists of the queries - from each template just a single one);&lt;/li&gt;
  &lt;li&gt;reduced statistics creation (do not keep redundant statistics but share them among different configurations/physical designs within a configuration);&lt;/li&gt;
  &lt;li&gt;exploit the test server to reduce the load on the production server (extend DTA to specify for a simulation: workload, configuration, and also the underlying hardware, e.g., specify number of CPUs on which to run the workload or the available memory).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The key word is simulation - do not copy any data for tuning or do not create new physical designs for testing the performance but imitate the structures, data, use sampling effectively and leverage the “what-if” type of analysis as well as query optimizer plans and costing.&lt;/p&gt;

&lt;h2 id=&quot;extensions-added-to-dta&quot;&gt;Extensions added to DTA:&lt;/h2&gt;

&lt;h3 id=&quot;how-to-provide-the-workload-for-tuning&quot;&gt;How to provide the workload for tuning?&lt;/h3&gt;

&lt;p&gt;The workload is a set of SQL statements that execute against the database server. A workload can be obtained by using SQL Server Profiler, a tool for logging events that execute on a server. In the current version of DTA, the Query Store can be used to collect the information about the workload for the tuning purposes (and it retains the information between server restarts): &lt;a href=&quot;https://docs.microsoft.com/en-us/sql/relational-databases/performance/tuning-database-using-workload-from-query-store?view=sql-server-2017#how-to-tune-a-workload-from-query-store-in-dtaexe-command-line-utility&quot;&gt;How To Tune a Workload from Query Store in dta.exe command line Utility?&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-main-benefits-of-keeping-the-tuning-advisor-in-syncstep-with-the-query-optimizer&quot;&gt;The main benefits of keeping the tuning advisor in sync/step with the query optimizer:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;The recommended physical design will be (with a very high probability) used by the query optimizer.&lt;/li&gt;
  &lt;li&gt;Query optimizer cost model evolves over time, which makes the costing for hypothetical physical designs probably better.&lt;/li&gt;
  &lt;li&gt;Via the query optimizer, we also take into account the hardware environment where queries will be executed (this hardware environment can be simulated, thus we can run the workload on a test server and tune it there for the environment of the production server).&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;recap-from-the-previous-papers-on-dta-falstart-greedy-search-schema-mk&quot;&gt;Recap (from the previous papers on DTA): Falstart greedy search schema \((m,k)\).&lt;/h3&gt;

&lt;p&gt;If we are limited to only k indexes, but our tuning recommends many more, than we have to narrow down the selection. It can be done by first choosing a small number \(m\) of indexes, this is the best possible configuration of m indexes. Then, we add a single index at a time, choosing the one that decrease the overall workload cost the most, in a greedy fashion, until we reach the configuration with the required \(k\) indexes.&lt;/p&gt;

&lt;h3 id=&quot;steps-of-workload-tuning-in-dta&quot;&gt;Steps of workload tuning in DTA:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Column group restriction (a pre-processing step): similar to the data mining algorithm of frequent itemsets - aim is to find &lt;em&gt;interesting&lt;/em&gt; columns, eliminate from consideration columns that can have only a marginal impact on the final configuration (e.g. take only the columns that appear in the queries, let’s say, at least 2 times).&lt;/li&gt;
  &lt;li&gt;Candidate selection: choose the best configuration (set of indexes, materialized views, partitioning) for each query in the workload separately. The configurations’ cost is based on the “what-if” analysis carried out by the query optimizer.&lt;/li&gt;
  &lt;li&gt;Merging: take into account the whole workload - especially the modification queries and the storage footprint constraints, propose another set of candidates (of indexes or materialized views) that could benefit the whole workload.&lt;/li&gt;
  &lt;li&gt;Enumeration: finally consider all the candidates (from the candidate selection and the merging steps) to produce the final configuration. the Greedy \((m,k)\) schema is used for this purpose.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;scaling-to-large-workloads-3-options&quot;&gt;Scaling to large workloads, 3 options:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;sample queries from the workload - this ignores how costly the queries are;&lt;/li&gt;
  &lt;li&gt;Tune the top k most expensive queries - many workloads are templatizes - so you might end up catering only to a single template.&lt;/li&gt;
  &lt;li&gt;Find signatures of the queries (basically templates behind the queries) and then tune for the set of templates (with an example of a single query per template). Two queries have the same signature if they are identical in all respects except for the constants referenced in the query.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;tuning-on-the-test-server&quot;&gt;Tuning on the test server&lt;/h3&gt;

&lt;p&gt;Copy only the metadata about the databases from the production to the test server. If some statistics for an index are missing, then create them on the production server (this is the biggest overhead). The main idea is to simulate as much as possible and reduce the load on the production server by running the tuning on the test server. Moreover, the production environment can be simulate on the test server, by specifying a specific hardware (number of CPUs, memory available) for the tuning on the test server.&lt;/p&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments:&lt;/h3&gt;
&lt;p&gt;Overall, the experiments are comprehensive. It is worth mentioning that the DTA does a better job than some DBAs. For example, for workload from one of the customers, the hand-tuned design was worse than a baseline (only indexes to enforce primary/uniquer keys) due to presence of updates. For this workload, DTA correctly recommended no additional physical design structures. For the TPC-H benchmark with 10GB of the input data, the size of the raw data in database was 12.8 GB. After the tuning, the expected performance improvement based on the costing from the query optimizer should increase by 88% but the decrease in the total workload execution time was 83%. The total storage space alloted was three times the raw data size. The compression of the workload (based on the templates and the signatures) provided even 40X improvement in tuning time for workloads with many query templates with a small degradation in the quality (0.5% or 1%). However, for the workload without any templates, such as TPC-H, the reduction in the tuning time was 1% (with, of course, no decrease in the quality - perhaps the number of extracted templates was equal to the total number of queries in the TPC-H benchmark, which is 22).&lt;/p&gt;

&lt;h3 id=&quot;limitations&quot;&gt;Limitations:&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Microsoft SQL Server 2005 supports only a single-column range partitioning. It was improved later on slightly:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Bad News&lt;/em&gt;: The partition function has to be defined on a single column;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Good News&lt;/em&gt;: That single column could be a persisted computed column that is a combination of the two columns you’re trying to partition by.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Query optimizer (on which the physical recommendations are made) do not model all the aspects of query execution, e.g., the impact of indexes on locking behavior.&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Mon, 30 Apr 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/04/30/database-tuning-advisor-for-microsoft-sql-server-2005.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/04/30/database-tuning-advisor-for-microsoft-sql-server-2005.html</guid>
        
        <category>database Microsoft msr DTA SQL Server tuning advisor DBMS vldb 2004</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>An Efficient, Cost-Driven Index Selection Tool for Microsoft SQL Server</title>
        <description>&lt;h1 id=&quot;an-efficient-cost-driven-index-selection-tool-for-microsoft-sql-server-vldb-1997&quot;&gt;An Efficient, Cost-Driven Index Selection Tool for Microsoft SQL Server (VLDB 1997)&lt;/h1&gt;

&lt;h2 id=&quot;these-are-my-notes-on-the-paper&quot;&gt;These are my notes on the paper:&lt;/h2&gt;

&lt;p&gt;The main approch is not to use any 1) query-syntax-based method or 2) knowledge base with best practices method but instead 3) re-use the optimizer with its what-if mode to cost potential physical designs (in this case - only indexes). The main focus in on how to recommend a decent set of indexes (a.k.a. a configuration) in a reasonable amount of time. To this end a 3 techniques are applied:&lt;/p&gt;

&lt;h3 id=&quot;candidate-index-selection---choose-the-best-index-for-each-query-separately&quot;&gt;Candidate index selection - choose the best index for each query separately&lt;/h3&gt;

&lt;p&gt;Determine the best configuration for each query independently. Treat each query from the total wokload, as a separte workload. If the query is read-only, then usually the indexes covering all admissible columns (which are used in the where or group by clauses) are chosen. If a query contains some modifications, then the maintenance of the index is taken into account and it might be the case that no index is recommended for the query.&lt;/p&gt;

&lt;h3 id=&quot;configuration-enumeration-via-a-falstart-greedy-algorithm&quot;&gt;Configuration enumeration via a “falstart-greedy” algorithm&lt;/h3&gt;

&lt;p&gt;The candidate index selection step gives a set of indexes for each query, then takes the union of them and considers the configuration for the whole workload. If there is a constraint of max k indexes then we have to prune the set of all indexes. We do it via a greedy algorithm. (m,k) greedy algorithm: m - is a seed denoting a number &amp;lt;= k, thus we start with some number m of indexes (heuristic and experiments claim that it is good to start from m = 2). This m indexes are chosen by full enumeration of all possible m indexes, and selecting an optimal set of m indexes. From this point on (after full search of the best possible m indexes), we greedily try to add an index at a time, in each step trying to find the index that would end up with the biggest total reduction of the cost of the workload.&lt;/p&gt;

&lt;h3 id=&quot;consider-multi-column-indexes-eg-a-b-tree-on-two-columns&quot;&gt;Consider multi-column indexes (e.g. a B+ tree on two columns)&lt;/h3&gt;

&lt;p&gt;An iterative approach is adopted here - it is called MC-LEAD. Take into account multi-column indexes of increasing width. Intution is that for a two-column indexes to be desirable, a single-column index on its leading column must be desirable. MC-LEAD - a 2-column index is selected only if its leading column was in a winning 1-column index (an index chosen from the set of all 1-column indexes considered), the second column in the 2-column index must be admisible (the 2nd column is part of the where or a group by clause in a query).&lt;/p&gt;

</description>
        <pubDate>Sun, 29 Apr 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/04/29/efficient-cost-driven-index-selection-for-SQL-Server.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/04/29/efficient-cost-driven-index-selection-for-SQL-Server.html</guid>
        
        <category>databaes vldb microsoft 1997 auto-admin tuning DBMS</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>Cognitive databases</title>
        <description>&lt;h2 id=&quot;notes-on-papers-cognitive-databases-that-is-on-using-embeddings-to-extend-relational-query-processing&quot;&gt;Notes on papers: &lt;a href=&quot;https://arxiv.org/find/cs/1/au:+Shmueli_O/0/1/0/all/0/1&quot;&gt;cognitive databases&lt;/a&gt;, that is on using embeddings to extend Relational Query Processing.&lt;/h2&gt;

&lt;h2 id=&quot;general-idea&quot;&gt;General idea&lt;/h2&gt;

&lt;p&gt;Copy data from relational tables into pure text (the transformation from a database content into the text ready for NLP processing can be done in many ways). For each word in the text corpus, create a vector (word embedding). This enables us to leverage many NLP tools/techniques to enhance SQL queries via UDFs and providing more semantic understanding of data.&lt;/p&gt;

&lt;p&gt;They want to extract and use latent semantic information from all the database entities.
In other words, application of NLP (Natural Language Processing) based machine learning techniques for enhancing and answering relational queries.&lt;/p&gt;

&lt;p&gt;Deep Dive used machine learning to convert input unstructured documents into a structured knowledge base. This paper proposes an opposite technique - flatten the database content and use it as a text corpus.&lt;/p&gt;

&lt;h2 id=&quot;5cs&quot;&gt;5C’s:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Category: data mining on top of a database.&lt;/li&gt;
  &lt;li&gt;Context: how to provide semantic understanding of a database content.&lt;/li&gt;
  &lt;li&gt;Correctness: only an overview of capabilities that are offered by NLP using the Word2Vec (word embedding) on top of relational tables without any significant system or theory contributions.&lt;/li&gt;
  &lt;li&gt;Contributions: show the idea of how to harness NLP tools in the context of database systems.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;main-notes&quot;&gt;Main notes:&lt;/h2&gt;

&lt;h4 id=&quot;example-of-queries&quot;&gt;Example of queries:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Query 1: give 10 people most related to Mark (the text for word embedding extracted only from the DBMS for this query)&lt;/li&gt;
  &lt;li&gt;Query 2: which are the most dangerous vacation packages (use external sources + database content)&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;their-argument-formulating-sql-queries-to-get-the-same-results-is-a-daunting-task&quot;&gt;Their argument: formulating SQL queries to get the same results is a daunting task.&lt;/h5&gt;

&lt;h4 id=&quot;new-capabilities-they-claim-the-following-unique-capabilities&quot;&gt;New capabilities, they claim the following unique capabilities:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;ability to build joint cross-modal semantic model from multi-modal data,&lt;/li&gt;
  &lt;li&gt;transparent integration of novel cognitive queries into existing SQL query infrastructure,&lt;/li&gt;
  &lt;li&gt;using untyped  vector representations to support contextual semantic (i.e. not value based) queries across multiple SQL data types,  and&lt;/li&gt;
  &lt;li&gt;ability to import an externally trained semantic model and apply it to enable querying a database using a token not present in the database.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;to-the-best-of-their-knowledge-none-of-the-current-industrial-academic-or-open-source-database-systems-support-these-capabilities&quot;&gt;To the best of their knowledge, none of the current industrial, academic, or open source database systems support these capabilities.&lt;/h4&gt;

&lt;h3 id=&quot;theoretical-aspects&quot;&gt;Theoretical aspects:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;word vectors can be used to compute the semantic and/or grammatical closeness of words as well as test for analogies&lt;/li&gt;
  &lt;li&gt;closeness of vectors is determined by using the cosine or Jacard distance measures between words - they do not use Jackard but only the cosine distance&lt;/li&gt;
  &lt;li&gt;very shallow representation of numbers - just as pure text 101.1 -&amp;gt; “101.1”&lt;/li&gt;
  &lt;li&gt;you can extend the vectors for a row with vectors from the row to which the foreign key corresponds to (natural and easy extension)&lt;/li&gt;
  &lt;li&gt;some additional operators/ UDF functions:&lt;/li&gt;
  &lt;li&gt;proximityMAX() given two sets of vectors as parameters, it returns the highest cosine distance between a vector from one set and a vector from the other set&lt;/li&gt;
  &lt;li&gt;proximityAVG() - generates two sets of tokens, one from its first and one from its second argument. Represents a set of tokens via a single vector which is the average of the vectors associated with the tokens in the set. Returns the cosine distance of the two vectors representing the two sets.&lt;/li&gt;
  &lt;li&gt;subsetProximityAVG() - semantically in between the proximityMAX() and proximityAVG(). Takes additional parameter: size. Considers all sets of tokens out of sequnence1 of cardinality size and computes an average vector for each such subset. The same is done for sequence2 (the 3rd parameter). Returns maximum of cosine distances between average vectors of subsets.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;systems-issues&quot;&gt;Systems issues:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;the vector training time can be very large -&amp;gt; use GPUs&lt;/li&gt;
  &lt;li&gt;high cost of accessing the trained vectors from the system tables; note that the text form of the database content (for the word embedding) can be much larger that the content of the database itself (because e.g. each word can be proceeded with a table:column name and expressed as a 200 dimensional vector)&lt;/li&gt;
  &lt;li&gt;the execution cost of CI (Cognitive Intelligence) queries is dependent on the performance of the distance function - their cosine distance is omnipresent and they did not really mention/use any other distance measure (apart from a single occurrence of the Jackard distance) between vectors in their&lt;/li&gt;
  &lt;li&gt;distance function and performance trade-offs: higher dimensional vectors (they use standard 200 dimensional vectors) give better semantic expressiveness but longer training time and higher cost of calculation of the distances between vectors (also increase the storage footprint)&lt;/li&gt;
  &lt;li&gt;view maintenance of vectors - the vectors and the text version of data is just a copy/another version of the data that has to be kept in sync with the main database content&lt;/li&gt;
  &lt;li&gt;reducing redundant computations (proposed in the paper): for a given vector, we can first identify a candidate set of vectors that are spatially closer to it in the d dimensional vector space using either locality sensitive hashing (LSH) or clustering via the Spherical K-Means algorithm. Both approaches can be accelerated using SIMD or GPUs.&lt;/li&gt;
  &lt;li&gt;they propose many other techniques how to make the NLP + DBMS synergy more viable by leveraging: distributed computing, multi-cores processing, SIMD and GPUs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-issues&quot;&gt;Other issues:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;the thresholds for their semantic queries - e.g. for the cosine similarity measure, users have to specify the value from -1 (dissimilar) to 1 (very similar) - but how would users know which is the one that they want or which would make sense? Authors’ answer - these  bounds are application dependent, much like hyperparameters in machine learning. One learns these by exploring the underlying database and running experiments. In the future, one can envision a tool that can guide users to select an appropriate numeric bound for a particular CI query.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;systems-used&quot;&gt;Systems used:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;the prototype was built on top of Spark - using Scala and Python&lt;/li&gt;
  &lt;li&gt;they used the famous Word2Vec for the work embedding&lt;/li&gt;
  &lt;li&gt;IBM Watson Visual Recognition System (VRS) for classification and text description&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;what-is-needed-a-benchmark-for-evaluating-and-ranking-ci-cognitive-intelligence-systems&quot;&gt;What is needed: a benchmark for evaluating and ranking CI (Cognitive Intelligence) systems.&lt;/h5&gt;

&lt;h2 id=&quot;two-more-things&quot;&gt;Two more things:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Updates of the text corpus and the word vectors. Authors of the papers don’t specify how the (copied) raw text corpus (from the tables) would be updated. In the first paper (from 2016), they mentioned that the word embedding would have to be re-trained, however in the latter paper (2017) they found that there is a possibility of incremental training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I think of the other approach. How can we leverage the existing database systems to propose schemas for unstructured data? We could flatten many databases to a text corpus and create the vectors from them with direct connection to the source schema, physical designs, etc. Then for a given unstructured input of data - let me say even to a subset of sources in a data lake - we could propose a schema (if we treat the data from the data lake as the text corpus and also create word embedding for it).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In general, I think that this is just an idea of extending DBMS with word embedding. For now, it does not have any further significant insights. They only use features offered by expressing words as vectors but has not extended it beyond any loosely coupled DBMS + Word2Vec. It might make more sense to write/claim a system to support Word2Vec on a large scale for truly big data than just flatten the database content.&lt;/p&gt;

</description>
        <pubDate>Sun, 29 Apr 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/04/29/cognitive-databases.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/04/29/cognitive-databases.html</guid>
        
        <category>databaes nlp word2vec word-embeddings IBM Watson</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>AutoAdmin What-if Index Analysis Utility</title>
        <description>&lt;h3 id=&quot;notes-on-a-paper-autoadmin-what-if-index-analysis-utility&quot;&gt;Notes on a paper: AutoAdmin “What-if” Index Analysis Utility&lt;/h3&gt;

&lt;p&gt;First, still even today the tools to analyze query performance in databases are not perfect. Let’s look at this comment from this website: https://goo.gl/6xxnS6:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A whole recommendation is not very helpful. It is often suggested to create N indices to achieve an X% improvement. Of these N indexes, almost always 1 or 2 are the indices most contributing, others weigh the database work without great gain. I suggest adding a column where we can see the contribution of each index to the X% improvement. To do this today, it takes a long time, because you need it reevaluate each suggested index one by one.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There was an effort in 1998 to provide a comprehensive visualisations of: workload, configurations and cost and index usage statistics.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Workload analysis: this consists of queries and their structural properties (query type: select/update/insert/delete, it it is an aggregation query (with the group by clause) or not, whether the query has nested subqueries, columns on which join conditions exists).&lt;/li&gt;
  &lt;li&gt;Configuration analysis: consists of indexes and their strucutral properties (size of the index, table on which the index is built, time of creation, whether or not the index is clustered).&lt;/li&gt;
  &lt;li&gt;Cost and index usage: the most important part of the system - exhibits relationship objects that capture the interaction between a specific configuration and queries in a workload.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;queries&quot;&gt;Queries&lt;/h3&gt;

&lt;p&gt;Users are provided with a specialized interface to query the hypthetical designs, however, in principle, they can write the SQL queries on their own against the analysis data tables.&lt;/p&gt;

&lt;h3 id=&quot;main-execution-estimate-configuration-set-of-indexes-for-a-query&quot;&gt;Main execution: estimate configuration (set of indexes) for a query&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Create all needed hypothetical indexes in the configuration.&lt;/li&gt;
  &lt;li&gt;Request the optimizer to:
    &lt;ol&gt;
      &lt;li&gt;Restirct its choice of indexes to those in the given configuration (the database catalog cannot be modified, as the other real queries might be running in the system, but this configuations can be specified on the users session level).&lt;/li&gt;
      &lt;li&gt;Consider the table and index sizes in the database to be adjusted by the scaling values (users can specify that a given table can grow in the future - as expected scale of the current size).&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Request the optimizer to produce the optimal execution plan for the query and gather the results:
    &lt;ol&gt;
      &lt;li&gt;the cost of the query&lt;/li&gt;
      &lt;li&gt;indexes (if any) used to answer the query&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;adaptive-sampling-strategy-for-creating-hypothetical-indexes&quot;&gt;Adaptive sampling strategy for creating hypothetical indexes&lt;/h3&gt;

&lt;h4 id=&quot;side-note&quot;&gt;Side note&lt;/h4&gt;

&lt;p&gt;An interseting side note is a short description of a sampling strategy (described in detail in another paper), but explained enough and worth mentioning here.&lt;/p&gt;

&lt;h4 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h4&gt;

&lt;p&gt;An adaptive page-level sampling algorithm is used to efficiently gather statistical measures relevant to query optimization. Incrementally, sample of size \(\sqrt(n) = m\) is drawn, where \(n\) is the number of pages in the table. The samples are stores in a sample-table, in sorted order. The statistical measures contain, for example, equi-depth histograms. After the first sample is drawn and the statistics gathered, we draw another sample and verify if the new sample has “similar” statistical characteristics as the previously sampled data stored in sample-table (for now, the table contains just a single sample of size m). To be precise, we check if the values in the new sample are divided approximately in equal numbers in each bin of the equi-depth histogram. If the statistics are in a certain ballpark similar, we finish the algorithm. Otherwise, the last sample is merged into the sample-table and the statistics are updated. We repeat the process until the aggreement in statistics between the sample-table and a newly drawn sample is reached, all we drawn some specified number of samples.&lt;/p&gt;

&lt;h2 id=&quot;main-takeaways&quot;&gt;Main takeaways&lt;/h2&gt;

&lt;p&gt;The index anslysis utility is indespensible - and this is what this paper addresses. Analyze the impact of current or hyptothetical indexes on the performance of your workload (set of queries). This approach is based on relative estimation of the cost that enables a large class of analysis at low cost. The authors implemented a tool that can use hypothetical indexes to predict workload cost, this was achieved by using sampling based techniques.&lt;/p&gt;

&lt;p&gt;Thus, the most useful command provided is: Esimtate Configuration (set of indexes) of &lt;workload_name&gt; for &lt;configuration_name&gt;&lt;/configuration_name&gt;&lt;/workload_name&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 29 Apr 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/04/29/auto-admin-what-if-index-analysis-utility.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/04/29/auto-admin-what-if-index-analysis-utility.html</guid>
        
        <category>databaes sigmod microsoft 1998 auto-admin tuning DBMS</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>The Data Flow model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-order Data Processing</title>
        <description>&lt;h1 id=&quot;the-data-flow-model-a-practical-approach-to-balancing-correctness-latency-and-cost-in-massive-scale-unbounded-out-of-order-data-processing&quot;&gt;The Data Flow model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-order Data Processing&lt;/h1&gt;

&lt;h2 id=&quot;these-are-my-notes-on-the-paper&quot;&gt;These are my notes on the paper:&lt;/h2&gt;

&lt;h3 id=&quot;strong-points&quot;&gt;Strong points:&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;The paper presents a new conceptual and principled model for stream processing. The main contribution is focus on the session windows - unbounded and unaligned windows with windowing based on a specific key. This was motivated by a use case where Google was receiving an influx of input events, that were just a single instances of specific users’ actions. However, the semantic of a session was needed to reason about users’ activities within a longer interaction with a system. The goal was achieved by the Window Merging operation with the following stages in the pipeline: Assign Windows, Drop Timestamps (preserve only the window spans), Group by key (e.g. user_id), Merge Windows and Group also by Windows (here is the case where we create single sessions for the users), Expand to Elements.&lt;/li&gt;
  &lt;li&gt;An important conceptual contribution is about the adaptability over time - we simply know that the watermarks are only logical and can be either incorrect (as lagging events from the past are coming later than expected) or simply too slow for the processing needs. Simply, the watermark is a global progress metric it can be held back for the entire pipeline by a single slow datum. The percentage of a watermark is a good trade-off. Processing majority of the data quickly is much more useful than processing 100% of the data more slowly - thus introducing percentile watermark triggers in the model. Moreover, having regularly updated, partial views on the data is much more valuable than waiting until mostly complete views are ready once the watermark passed the end of the session.&lt;/li&gt;
  &lt;li&gt;Clear distinction between the event time (as recorded by the source of the event) and the processing time (as recording in the processing system of the event).&lt;/li&gt;
  &lt;li&gt;The system has to be flexible and adjust to users’ need in terms of: cost, processing time (latency) and correctness.&lt;/li&gt;
  &lt;li&gt;The use quite refined language with phrases such as: “a similar dance occurs for the values 3, 8, and 1, …” (page 1801, last paragraph before Section 3).&lt;/li&gt;
  &lt;li&gt;They postulate the setup with a single implementation written in a unified model that could run in streaming and batch mode. In their case, the core windowing and triggering code is quite general, and a significant portion of it is shared across batch and stream implementations.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;weak-points&quot;&gt;Weak points:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;We do not get sufficient information about the engineering of systems that follow the Data Flow model (this paper was published in the industrial track).&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;more-comments-that-are-not-arranged-in-a-specific-order&quot;&gt;More comments that are not arranged in a specific order:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The system became an Apache Beam project: https://beam.apache.org/&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Paradigms:
    &lt;ol&gt;
      &lt;li&gt;Unified:  Use a single programming model for both batch and streaming use cases.&lt;/li&gt;
      &lt;li&gt;Portable: Execute pipelines on multiple execution environments.&lt;/li&gt;
      &lt;li&gt;Extensible: Write and share new SDKs, IO connectors, and transformation libraries.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This interface can sit on top of different data sources (an ambitious approach).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Interesting bits of information in the paper:
    &lt;ol&gt;
      &lt;li&gt;6 use cases that they had at Google that led them to the design.&lt;/li&gt;
      &lt;li&gt;Iintroduction: an amazing job of constructing the arguments.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When you write a paper for a conference you want to put in as many : no idiots statements - don’t say explicitly about what the reviewers might be worried about. Bunch of statements like that in the paper. Non-technical stuff - lots of people work on the problem (streaming systems) - they go through the full stream of systems: Niagra, Esper (the 1st open-source streaming system), Storm, Pulsar, Spark Streaming, Flink, they bucket them and try to say what they think is lacking in the systems - one sentence what the group did right and wrong.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Important excert: “None of these shortcomings are intractable, and systems in active development will likely overcome them in due time. But we believe a major shortcoming of all the models and systems  mentioned  above  (with  exception  given  to  CEDR and Trill), is that they focus on input data (unbounded or otherwise)  as  something  which  will  at  some  point  become complete.  We believe this approach is fundamentally flawed when  the  realities  of  today’s  enormous,  highly  disordered datasets clash with the semantics and timeliness demanded by consumers.  We also believe that any approach that is to have broad practical value across such a diverse and varied set of use cases as those that exist today (not to mention those  lingering  on  the  horizon)  must  provide  simple,  but powerful, tools for balancing the amount of correctness, latency, and cost appropriate for the specific use case at hand. Lastly, we believe it is time to move beyond the prevailing mindset of an execution engine dictating system semantics;
properly designed and built batch, micro-batch, and streaming systems can all provide equal levels of correctness, and all three see widespread use in unbounded data processing today.”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Logical data independence - this is mostly physical model independence. Query optimization. Abstraction - semantic of what is going to happen.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Leaky abstraction. The SQL language abstracts away the procedural steps for querying a database, allowing one to merely define what one wants. But certain SQL queries are thousands of times slower than other logically equivalent queries. On an even higher level of abstraction, ORM systems, which isolate object-oriented code from the implementation of object persistence using a relational database, still force the programmer to think in terms of databases, tables, and native SQL queries as soon as performance of ORM-generated queries becomes a concern.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;That’s not a really useful way to think about the unbounded data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On a given database, think about a database at a given time. More data is continually coming in.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You can be more efficient, in reality, unbounded datasets have been processed using repeated runs of batch systems since their conception, and well-designed streaming systems are perfectly capable of processing bounded data. From the perspective of the model, the distinction of streaming or batch is largely irrelevant, and we thus reserve those terms exclusively for describing runtime execution engines.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jujutsu (/dʒuːˈdʒuːtsuː/ joo-JOOT-soo; Japanese: 柔術, jūjutsu About this sound listen (help·info)), also known in the West as Ju-Jitsu or Jiu-Jitsu, is a Japanese martial art and a method of close combat for defeating an armed and armored opponent in which one uses either a short weapon or none.
In the paper: some Jujutsu moves - take the attack and claim this is exactly what we do.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another interesting excerpt: “It is lastly worth noting that there is nothing magical about this model. Things which are computationally impractical in existing strongly-consistent batch, micro-batch, streaming, or Lambda Architecture systems remain so, with the inherent constraints of CPU, RAM, and disk left stead-fastly in place.” Of course, there is nothing magical here!!!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In terms of form - no evaluation (industrial paper), incredibly well set up framework in the introduction - it’s slightly cynical (believing that people are motivated by self-interest; distrustful of human sincerity or integrity).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Defensive arguments in the introduction. It’s a very crowded space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;They try to come up with a comprehensive model. Can we modify different types of streaming, window, engine semantics - rather than to pick one and claim that this is how it should be. However, you have to give up some performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Understand patterns/anti-patterns for your own research.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MapReduce was rejected by SIGMOD and VLDB. Finally, it was accepted at the OSDI.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Meta-information: about stream processing - we have several requirements - semantics and performance. Can we separate - set them independently - the upper level - a semantic buffer - buffer for the tuples - whether it’s a window. Lower part is an incremental view maintenance. This paper does not have an engine included, Apache Beam has no engine. The engine is closed-source. Given an engine and semantic - engine fault? if not a semantic concept supported. How can you build a better system? Paraphrase: can you really decouple the semantics from the performance with the engine. Non-stream query.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Beam can sit on top of Spark streaming. Spark streaming is built on top of Spark and RDDs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If Beam become the most popular streaming interface - the typical things that people do with it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It’s also about missing data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Incorporate semantic into the underlying engine.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The narrow waist architecture - abstraction layer, the semantic level on top of it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;User facing semantics: usually SQL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Underlying Relational Algebra with a bunch of extensions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Different notions - SQL - logical views - the notion of logical consequences.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Logical data independence - through views.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Between relational algebra and engine implementation is the physical independence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Views hide the underlying database schema.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Plug in a new engine - it should support the relational algebra ++.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When the relational model was proposed. Conventional wisdom - a baeutiful academic exercise - the great debate - very early SIGMOD - relational people - and people who did an ad-hoc stuff. Ingres and system R - we think that it’s a right model. The first implementation of the relational algebra. Some parallels with this and programming language folks. In the initial days - high level languages - you need to write hand tuned assembly - none really does it. With the PL folks and the idea of a high level language. The underlying architecture did not change that much. Problem - most architecture people would disagree with that. The idea of RDD is very different from the traditional tables in databases. Column store is overhaul from the row-store. Look at a query and figure out an optimal course of action. In a long run, abstraction wins out. The hard coded solution breaks. IRS crushes on the tax day - they were running on the 30 old system. Burning platform - it’s gonna run and burning. Looking at these figure. You are in favor of BigDAWG - there is some reality to it. BEAM is a one-size fits all solution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-core came up - leaky abstraction open MP - parallelize - cache conscious algorithms - you should not worry about the stuff. You have to have a decent system and big market share so that everybody will follow you.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Spark - solve 80% of streaming problems that people have. It’s one of the constant struggles in systems - where do you draw the abstraction. Architecture - at some point, the underlying system is too complicated - the abstraction is too big.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Programmers don’t kmow enough to provide setting to the machine - instead specify to the requirements of the system. There is gonna be some problem with the cache conscious problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Which abstraction catch on - where do you draw them? Ingres QUEL language much better than SQL.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;technical-stuff&quot;&gt;Technical stuff:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Fundamental thing on which their system is based is the notion of windows - unbounded stream of data and you need to process it - you have to delineate the blocks of data. Jennifer Widom - http://ilpubs.stanford.edu:8090/758/&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CQL (2003), a Continuous Query Language, is supported by the STREAM prototype Data Stream Management System at Stanford. CQL is an expressive SQL-based declarative language for registering continuous queries against streams and updatable relations. We begin by presenting an abstract semantics that relies only on ``black box’’ mappings among streams and relations. From these mappings we define a precise and general interpretation for continuous queries. CQL is an instantiation of our abstract semantics using SQL to map from relations to relations, window specifications derived from SQL-99 to map from streams to relations, and three new operators to map from relations to streams. Most of the CQL language is operational in the STREAM system. We present the structure of CQL’s query execution plans as well as details of the most important components: operators, inter-operator queues, synopses, and sharing of components among multiple operators and queries. Examples throughout the paper are drawn from the Linear Road benchmark recently proposed for Data Stream Management Systems. We also curate a public repository of data stream applications that includes a wide variety of queries expressed in CQL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Windowing:
    &lt;ol&gt;
      &lt;li&gt;fixed window&lt;/li&gt;
      &lt;li&gt;sliding window - each window defines a relation - run SQL on the separated relation -&lt;/li&gt;
      &lt;li&gt;extensible window - gunning tally of the window - at some&lt;/li&gt;
      &lt;li&gt;session window&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;The window parameters:
    &lt;ol&gt;
      &lt;li&gt;time domain: event and processing time&lt;/li&gt;
      &lt;li&gt;a) accumulating vs. b) discard vs. c) accumulating and retracting - what do you do for the new windows in terms of the past data&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;More details:
    &lt;ol&gt;
      &lt;li&gt;Accumulate - add to my previous value.&lt;/li&gt;
      &lt;li&gt;Discard - run the relational algebra - I get a new answer and discard the old one.&lt;/li&gt;
      &lt;li&gt;Accumulate and retract - ignore the previous value and treat the new value as the source of the truth.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Processing time - these two times are relevant - skew between them changes as the system runs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fundamental statement: you can never rely on a notion of completeness. Conceptually - you need to keep every window - every window anybody has asked for.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Retract - when you hit the end of the window - send an anti-result - do the - if a past was 7, and new version is 8, then first send -7 and then they send 8.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Accumulating takes some additional space. The whole thing backs to the application question - clear answer - there is no magic - if data is not there, data is not there. Semantics - some hope of understanding - at any time stuff can show up.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Watermarks - everything up to this point in time already appeared. You can define your own watermark. Accumulating vs. discarding vs. retract and accumulate. Element based windows - account window - every 3 records are windowed - whatever order they come - just group them. The window can be a predicate. You can watch some stock ticker. One quote and the next quote. Unclear whether they support that in their case.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Session semantic - virtual stream over a specific key - per user.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sessions are windows that capture some period of activity over a subset of the data, in this case per key. Typically they are defined by a timeout gap. Any events that occur within a span of time less than the timeout are grouped together as a session. Sessions are unaligned windows. For example, Window 2 applies to Key 1 only, Window 3 to Key 2 only, and Windows 1 and 4 to Key 3 only.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;From the first time I see the user - and the default activity window is 30 minutes, if nothing happens I close the session.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Watermark - the idea of repeat clause. Can combine the late data after the watermark. Some point at which I’m pretty sure/confident I saw close to 100% of the data. Problems, too fast - a lot of data show up after the watermark - exception handling. They might be too slow - one solution to the late data is that you wait - their fundamental assumption is that you never know. Stupid argument - too fast - it doesn’t work very well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The notion of watermark - adapt over time and percentile watermark triggers in the model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Streaming used for real time recommendation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Example section: Processing time (y axis), event time (x axis) - when things really happen, processing time lags event time. They show values instead of when the event happened. Ideal - processing time aligned with the event time. The skew between the ideal time and the processing time. Watermarks are used to trigger the operations on the windows. For the system - how your actual watermark is generated - aggressive enough! Trade-off between how exact you’re and how you generate the results. Confidence - how much - the probability of more data showing up.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;6 different ways how we see the streaming system at Google. Big joins of the log tables - the answers were so wrong we couldn’t trust it, sessioning thing, billing (fact that you had to deal with the old data), statistics collection (do not need the exact answer), recommendation engine does not have to be perfect, stock market example - based on the shape of the data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Apache Beam ~ PL1 - include every language that somebody ever came up with.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 26 Apr 2018 00:00:00 +0200</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/04/26/DataFlowModelGoogleSIGMOD2015.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/04/26/DataFlowModelGoogleSIGMOD2015.html</guid>
        
        <category>databaes streaming google sigmod 2015</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>Log barrier function</title>
        <description>&lt;p&gt;We are given an oracle access to a function value \(f(x)\), its derivative \(\nabla f(x)\) and the hessian \(\nabla^2 f(x)\). Based on this provided values, we can easily find the value \(g(x) = - log f(x)\), derivative \(\nabla g(x)\) and hessian \(\nabla^2 g(x)\).&lt;/p&gt;

&lt;p&gt;First, we use the chain rule:
\(\begin{gather*}
  \nabla^2 (-log(f(x))) = - \nabla ( \nabla log (f(x))) = \nabla \frac{-\nabla f(x)}{f(x)}
\end{gather*}\)&lt;/p&gt;

&lt;p&gt;Next, we use the quotient rule:
\(\begin{gather*}
  \frac{-\nabla^2f(x) f(x) + \nabla f(x) \nabla f(x)^T}{f(x)^2} =
  \frac{-\nabla^2f(x)}{f(x)} + \frac{\nabla f(x)\ \nabla f(x)^T}{f(x)^2}
\end{gather*}\)&lt;/p&gt;

\[\begin{gather*}
\nabla g(x) = \frac{-\nabla f(x)}{f(x)}
\end{gather*}\]

\[\begin{gather*}
\nabla^2 g(x) = \frac{-\nabla^2f(x)}{f(x)} + \frac{\nabla f(x)\ \nabla f(x)^T}{f(x)^2}
\end{gather*}\]
</description>
        <pubDate>Thu, 22 Feb 2018 00:00:00 +0100</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/02/22/log-barrier.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/02/22/log-barrier.html</guid>
        
        <category>log-barrier optimization convexity interior-point-method</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>Fastests mixing Markov chain on a graph</title>
        <description>&lt;p&gt;I talked to my friend who studied statistics about my course on convex optimization. He asked about a type of problems that we are trying to solve in the course. The first one that came to my mind was on fastest mixing Markov chain on a graph. I think that I there are many bits and pieces that are really required to fully understand the problem (depending on your background), but I will try to dissect the problem to the smallest possible parts to make each of them clear, and finally show you the whole picture of beautiful Markov chains combined with convex optimization.&lt;/p&gt;

&lt;p&gt;The idea is that we are provided an undirected graph \(G=(V,E)\) where \(V\) represents vertices and \(E\) edges. The problem is to find the probability transition matrix \(P\) that minimizes the time until which we attain the equilibrium dirstribution for the Markov chain (which means that after a few jumps in the Markov chain, you don’t know where the next jump could end, i.e. on which node). In other words (plainly), a Markov chain is a graph in which with pair of vertices we associate probability of transition from one vertex to another. In a typical case, the transition probabilities can be different between the same vertices, depending on which direction we jump, thus it is usually the case: \(P_{ij} \ne P_{ji}\). However, in this problem we assume that the graph is undirected and the probabilities of jumps between all pairs of vertices are the same: \(P_{ij} = P_{ji}\). We also allow self-loops, so you can jump out from a vertex and jump in to the same vertex in a transition, this is associated with the probability \(P_{ii}\).&lt;/p&gt;

&lt;p&gt;What are our assumptions? The Markov chain has \(n\) vertices and we define the current state by \(X(t) \in {1,2,...,n}\), where \(t\) is a non-negative integer (you can think about it as a time step which can be much larger than n, as it expresses which node we were occuping at a given point in time). The probablities for each vertex sum up to \(1\) (you add all probabilities associated with edges connected to the given vertex). More formally:&lt;/p&gt;

&lt;p&gt;This is how we define the transition probability matrix \(P\):
\(P_{ij} = \text{probability} (X(t+1) = i | X(t) = j), \ i,j = 1,...,n\)&lt;/p&gt;

&lt;p&gt;As all probabilities, they have to be at least 0 (we add another constraint that for a given vertex all the probabilites of jumps add up to 1 so it implies that for a given vertex, a sinlge probability has to be at most \(1\), we don’t add the it here that \(P_{ij} \le 1\) so that our constraints are not redundant):
\(P_{ij} \ge 0\)&lt;/p&gt;

&lt;p&gt;We can pose this formulation as an optimization problem:
\(\min_{P \in S^n, t \in R} t \\
\forall_{i} \sum_j P_{ij} = 1 \ \ (P1 = 1) \\
\forall_{ij} P_{ij} \ge 0 \ \ (P \ge 0) \\
\forall_{ij \not\in E} P_{ij} = 0 \\
\forall_{ij} P_{ij} = P_{ji} \ \ (P^T = P) \\
-tI \preceq P - \frac{1}{n}11^T \preceq tI \\\)&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;In short, given an undirected graph \(G(V,E)\) on \(V = {1,...,n}\) we want to construct a random walk \(X(t)\) on the graph with symmetric transitions: $$P(X(t+1) = j&lt;/td&gt;
      &lt;td&gt;X(t) = i) = P_{ij} = P_{ji} \(that minimizes the \textit{mixing time}, the time by which\)X(t)\(is approximately uniform and independent of\)X(0)\(. Eigenvalues of matrix\)P: 1 = \lambda_1 \ge \lambda_2 \ge … \ge \lambda_n \ge -1 \(. Hnce, 1 is eigen-vector of\)P\(with eigenvalue\)1$$.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The main outcome is that the smaller the second eigenvalue (closer to 0), the faster the mixing time (or we want to maximize $$-\lambda_n).&lt;/p&gt;

&lt;p&gt;Claim: mixing time \(\propto - \frac{1}{log(max(\lambda_2,-\lambda_n))}\). We want to minimize \(max(\lambda_2, -\lambda_n)\).&lt;/p&gt;

&lt;p&gt;Fastests mixing Markov chain on a graph, credits: Prof. Stephen Boyd for his book on convex optimization, Prof. Nati Srebro (for the main introduction to the problem), Blake Woodworth (for explaining why we have to normalize eigenvectors), Angela Wu (for explaining how to isolate eigenvalues).&lt;/p&gt;
</description>
        <pubDate>Fri, 09 Feb 2018 00:00:00 +0100</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/02/09/Fastest-Mixing-Markov-Chain-On-A-Graph.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/02/09/Fastest-Mixing-Markov-Chain-On-A-Graph.html</guid>
        
        <category>markov</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>Notes on a paper: Multi-Scale Convolutional Neural Networks for Time Series Classification</title>
        <description>&lt;ol&gt;
  &lt;li&gt;Category: a research prototype of harnessing CNNs to analyze time-series data.&lt;/li&gt;
  &lt;li&gt;Context: deep learning application for time-series. Related to papers on machine/deep learning and time-series analysis (data mining and management). The paper has a theoretical basis with comprehensive analysis on the UCR data set.&lt;/li&gt;
  &lt;li&gt;Correctness: it assumes that the classification of time-series can be done more efficiently than with other methods for the time-series data.&lt;/li&gt;
  &lt;li&gt;Contributions: new architecture of CNNs for the TSC (Time-Series Classification).&lt;/li&gt;
  &lt;li&gt;Clarity: the paper is well written in general - a little bit less attention was given to the experimental part in terms of description.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are 3 types of classifiers for the time-series data:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Distance based&lt;/strong&gt;: the simplest one use Euclidean distance or DTW (aligns two time-series with dynamic warping) to measure the distance between time-series with 1KNN classification.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Feature based&lt;/strong&gt;: each time-series is characterized with a feature vector, SAX (Symbolic Aggregation approXimation) is a typical example where time-series is discretized (divided in windows) based on piece-wise mean value.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ensemble methods&lt;/strong&gt;: combine many different classifiers to achieve higher accuracy. The weight of distinct classifiers in an ensemble can be assigned based on the cross validation accuracy. The flat Collective Of Transform-based Ensemble (COTE) is an ensemble of 35 classifiers based on features from time and frequency domains.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The shaplets - signature subsequences of time-series. The shaplets are features that can be pre-determined a searched for in a new time-series.&lt;/p&gt;

&lt;p&gt;A key reason for the success of CNNs is its ability to automatically learn complex feature representations using its convolutional layers. The paper shows that it is possible to automatically learn the feature representation from time series. It was shonw that the feature based classfier (LTS - Learning shaplets) can be seen as a special case of convolution. Is a simple way they show that the Euclidean distance used in LTS between a shapelet (a features subsequence that can appear in a time-series) and a new time-series that has to be classified can be expressed as a convolution with added Euclidean (L2) norm of the part of the new time-series and a constant L2 norm (arbitrarily chosen) for a filter (kernel). Let start from notation:&lt;/p&gt;

&lt;p&gt;\(T = \{t_1,t_2,...,t_n\}\) is a time series.
\(f = \{f_1,f_2,...,f_n\}\) is a filter.
1-dimensional discrete convolution (the filter is flipped thus it is a standard convolution and not a cross-correlation):
\((T \cdot f)[i] = \sum_{j=1}^{m} f_{m+1-j}t_{i+j-1}\)&lt;/p&gt;

&lt;p&gt;The expression of Euclidean dinstance between a shaplet and new-time series as a convolution:&lt;/p&gt;

\[||T,f||_{2}[i] = \sum_{j=1}^m(t_{i+j-1} - f_{m+1-j})^2 \\
  = \sum_{j=1}^{m}t^2_}i+j-1} + \sum_{j=1}^{m}f^2_{m+1-j} - 2 \sum_{j=1}^{m}t_{i+j-1}f_{m+1-j} \\
  = \sum_{j=1}^{m} t^2_{i+j-1} + \sum_{j=1}^mf_j^2 - 2(T \cdot f)[i]\]

&lt;p&gt;\(\sum_{j=1}^{m} t^2_{i+j-1}\) is a constant for each time-series (the L2 norm of the time-series).&lt;/p&gt;

&lt;p&gt;\(\sum_{j=1}^mf_j^2\) each filter is restricted to the same L2 norm.&lt;/p&gt;

&lt;p&gt;A question is how we can use convolution to apply different distance measures? If we can do that, then we can leverage the GPUs and faster parallel computation.&lt;/p&gt;

&lt;p&gt;The MCNN is a mutli-scale convolutional neural network with time-series as input and a class label as output. The main idea is to capture temporal patterns at different time-scales. The architecture is divided into 3 parts:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Transformation of the input time-series with 3 branches:
    &lt;ul&gt;
      &lt;li&gt;identity mapping (input is not changed)&lt;/li&gt;
      &lt;li&gt;down-sampling in time domain&lt;/li&gt;
      &lt;li&gt;spectral transformation in frequency domain&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Local convolutions (applied to each branch separately) with max pooling&lt;/li&gt;
  &lt;li&gt;Full convolution with max pooling and Softmax&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The number of parameters in the local convolutional layer was reduced by down-sampling the time-series instead of increasing the filter size.&lt;/p&gt;

&lt;p&gt;The power of CNNs is in processing a huge amount of data. The analyzed dataset from UCR archive is relatively small so data augmentation by slicing is performed on the training and test data. The data is divided into windows (about 90\%) of the initial time-series, each window is set with the same label as the intial time-series and added as a new data point.&lt;/p&gt;

&lt;p&gt;In terms of images, a simple two-cell/pixed filter can find edges in an image. A filter with two values for time-series \(f=[1,-1]\) gives a gradient between two neighboring points. MCNN is able to learn such filters. Filters are of different sizes and an example of 15 value filter is given - the max pooling applied after convolution with the filter gives a discriminative value that distinguishes between time-series belonging to 2 different classes. It’s impressive that a single convolution filter can already achieve high accuracy to classify a dataset.&lt;/p&gt;

&lt;p&gt;The grid-search was adopted for hyper-parameter tuning based on cross validation. The hyper-parameters in MCNN include the filter size, pooling factor, and batch size. The search space for the filter size is {0.05, 0.1, 0.2} which denotes the ratio of the filter length to the original time series length, the search space for the pooling factor is {2,3,5}, which denotes the number of outputs of max-pooling (the pooling is applied to 2,3,5 values from the time-series). Binomail and Wilcoxon signed rank tests are used to compare the models.&lt;/p&gt;

&lt;p&gt;A multi-channel CNN has been proposed in another paper to deal with multivariate time-series.&lt;/p&gt;

&lt;p&gt;MCNN outperforms a standard non-specialized CNN for time-series classification. However, it’s still less accurate than COTE ensemble classifier.&lt;/p&gt;

&lt;p&gt;Link to the original paper: https://arxiv.org/pdf/1603.06995.pdf&lt;/p&gt;

</description>
        <pubDate>Mon, 05 Feb 2018 00:00:00 +0100</pubDate>
        <link>https://adam-dziedzic.github.io//html/2018/02/05/Multi-Scale-Convolutional-Neural-Networks-For-Time-Series-Classification.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2018/02/05/Multi-Scale-Convolutional-Neural-Networks-For-Time-Series-Classification.html</guid>
        
        <category>cnn time-series</category>
        
        
        <category>HTML</category>
        
      </item>
    
      <item>
        <title>3 Steps (2 minutes) to Setup Your Personal Website with Jalpc</title>
        <description>&lt;p&gt;Everyone wants to have a personal website, you can display your infomation to public, post blogs and make friends. If you are CS engineer, haveing a self website will benefit your interview.&lt;/p&gt;

&lt;p&gt;So, if you like this website &lt;a href=&quot;https://jarrekk.github.io/Jalpc/&quot;&gt;https://jarrekk.github.io/Jalpc/&lt;/a&gt; or &lt;a href=&quot;http://www.jarrekk.com&quot;&gt;http://www.jarrekk.com&lt;/a&gt; and are willing to have a website, here is a way to build your website in 3 steps(2 minutes). Following are steps to setup your website(make sure you have basic knowledge of &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; and &lt;a href=&quot;https://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt;, if you want to custom css/js &lt;a href=&quot;https://github.com/npm/npm&quot;&gt;NPM&lt;/a&gt; is needed):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Fork &lt;a href=&quot;https://github.com/jarrekk/Jalpc&quot;&gt;this project – Jalpc&lt;/a&gt; at &lt;a href=&quot;https://github.com&quot;&gt;GitHub&lt;/a&gt;. If you want to edit website at github, do it as following gif or clone forked repository. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git clone git@github.com:github_username/Jalpc.git&lt;/code&gt;.&lt;/p&gt;

    &lt;p&gt;&lt;!-- ![edit](static/assets/img/blog/3steps/edit.gif) --&gt;
 &lt;img src=&quot;static/assets/img/blog/3steps/edit.gif&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Enter into repository directory and edit following file list:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;_config.yml&lt;/strong&gt;: edit ‘Website settings’, ‘author’, ‘comment’ and ‘analytics’ items.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;_data/landing.yml&lt;/strong&gt;: custom sections of index page.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;_data/index/&lt;/strong&gt;: edit sections’ data to yours at index page, please notice comment at each file.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;_data/blog.yml&lt;/strong&gt;: edit navbar(categories) of blog page, if you have different/more blog page, copy &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;blog/python.html&lt;/code&gt; and change it to your category HTML file, and edit &lt;strong&gt;Python&lt;/strong&gt;, &lt;strong&gt;/python/&lt;/strong&gt; to your category name at items &lt;strong&gt;title&lt;/strong&gt; and &lt;strong&gt;permalink&lt;/strong&gt;, make sure title is the same as permalink but capitalized first letter(except HTML).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;CNAME&lt;/strong&gt;: If you wanna release website at your own domain name: edit it and create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gh-pages&lt;/code&gt; branch; if you want to use &lt;em&gt;github_username.github.io&lt;/em&gt;: leave it blank.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Go to repo’s settings panel, config &lt;strong&gt;GitHub Pages&lt;/strong&gt; section to make sure website is released.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Push changes to your github repository and view your website, done!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From now on, you can post your blog to this website by creating md files at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;post/&lt;/code&gt; directory and push it to GitHub, you can clear files at this directory before you post blogs.&lt;/p&gt;

&lt;p&gt;If you like this repository, I appreciate you star this repository. Please don’t hesitate to mail me or post issues on GitHub if you have any questions. Hope you have a happy blog time!😊&lt;/p&gt;
</description>
        <pubDate>Tue, 31 Jan 2017 00:00:00 +0100</pubDate>
        <link>https://adam-dziedzic.github.io//html/2017/01/31/3-steps-to-setup-website-with-Jalpc.html</link>
        <guid isPermaLink="true">https://adam-dziedzic.github.io//html/2017/01/31/3-steps-to-setup-website-with-Jalpc.html</guid>
        
        <category>Jalpc</category>
        
        <category>Jekyll</category>
        
        
        <category>HTML</category>
        
      </item>
    
  </channel>
</rss>
