@inproceedings{dziedzic2015bigdawg,
    title = {BigDAWG: a Polystore for Diverse Interactive Applications},
    author = {Dziedzic, Adam and Duggan, Jennie and Elmore, Aaron J. and Gadepally, Vijay and Stonebraker, Michael},
    booktitle = {DSIA (IEEE Viz Data Systems for Interactive Analysis)},
    year = {2015}
}

@inproceedings{dziedzic2016dbms,
    title = {DBMS Data Loading: An Analysis on Modern Hardware},
    author = {Dziedzic, Adam and Karpathiotakis, Manos and Alagiannis, Ioannis and Appuswamy, Raja and Ailamaki, Anastasia},
    booktitle = {ADMS (Accelerating analytics and Data Management Systems)},
    year = {2016}
}

@inproceedings{dziedzic2016transformation,
    title = {Data Transformation and Migration in Polystores},
    author = {Dziedzic, Adam and Elmore, Aaron and Stonebraker, Michael},
    booktitle = {HPEC (IEEE High Performance Extreme Computing)},
    year = {2016},
    organization = {IEEE}
}

@inproceedings{mattson2017demonstrating,
    title = {Demonstrating the BigDAWG Polystore System for Ocean Metagenomics Analysis.},
    author = {Mattson, Tim and Gadepally, Vijay and She, Zuohao and Dziedzic, Adam and Parkhurst, Jeff},
    booktitle = {CIDR (Conference on Innovative Data Systems Research)},
    year = {2017}
}

@inproceedings{dziedzic2019band,
    title = {Band-limited Training and Inference for Convolutional Neural Networks},
    author = {Dziedzic, Adam and Paparizzos, Ioannis and Krishnan, Sanjay and Elmore, Aaron and Franklin, Michael},
    booktitle = {ICML (International Conference on Machine Learning)},
    year = {2019}
}

@inproceedings{hendrycks-etal-2020-pretrained,
    title = "Pretrained Transformers Improve Out-of-Distribution Robustness",
    author = "Hendrycks, Dan  and
      Liu, Xiaoyuan  and
      Wallace, Eric  and
      Dziedzic, Adam  and
      Krishnan, Rishabh  and
      Song, Dawn",
    booktitle = " ACL (Association for Computational Linguistics)",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "ACL (Association for Computational Linguistics)",
    url = "https://aclanthology.org/2020.acl-main.244",
    doi = "10.18653/v1/2020.acl-main.244",
    pages = "2744--2751",
    abstract = "Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers{'} performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.",
}

@inproceedings{capc2021iclr,
    title = {CaPC Learning: Confidential and Private Collaborative Learning},
    author = {Christopher A. Choquette-Choo and Natalie Dullerud and Adam Dziedzic and Yunxiang Zhang and Somesh Jha and Nicolas Papernot and Xiao Wang},
    booktitle = {ICLR (International Conference on Learning Representations)},
    year = {2021},
    url = {https://openreview.net/forum?id=h2EbJ4_wMVq},
    abstract = {Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties.},
    video_url = {https://www.youtube.com/watch?v=0yWeY39cPLo},
    code_url = {https://github.com/cleverhans-lab/capc-demo},
    blog_url = {http://www.cleverhans.io/2021/05/01/capc.html},
    slides_url = {https://adam-dziedzic.com/static/assets/slides/capc2.pdf},
}


