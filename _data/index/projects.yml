# projects section data
# If you don't have language feature(language.yml is empty), ignore "i18n" items
# Suggest projects' img be located at 'static/assets/img/landing', and edit following img items.

- name: Collaborative Learning in ML
  #i18n: cpython
  url: "http://www.cleverhans.io/2021/05/01/capc.html"
  gh_user: cleverhans-lab
  repo: capc-demo
  img: static/assets/projects/capc3.png
  # desc: "Collaboration is a key component of learning, for both humans and machine learning. Doctors commonly ask their colleagues for diagnosis recommendations to improve their own, like referring patients to specialists. In machine learning, it is frequent to obtain better predictions by collaboratively polling the predictions of multiple machine learning models, a technique often known as ensemble learning. Such a collaboration can help improve a modelâ€™s performance by increasing the amount and breadth of training data available. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients, parameters, or other forms of model updates still contain private information. Furthermore, differentially private and federated learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. Confidential and Private Collaborative (CaPC) learning is the first method provably achieving both confidentiality and privacy in a collaborative setting by using techniques from cryptography and differential privacy literature."
  desc: "Confidential and Private Collaborative (CaPC) learning is the first method provably achieving both confidentiality and privacy in a collaborative setting by using techniques from cryptography and differential privacy literature."
  paper: static/assets/papers/capc.pdf
  slides: static/assets/slides/capc2.pdf
  talk_url: "https://youtu.be/0yWeY39cPLo"
  bibtex: static/assets/bibtex/capc.txt

- name: Band-limited Training and Inference For Convolutional Nerual Networks
  #i18n: cpython
  url: "http://proceedings.mlr.press/v97/dziedzic19a.html"
  gh_user: adam-dziedzic
  repo: bandlimited-cnns
  img: static/assets/projects/band-limit4.png
  desc: "The convolutional layers are core building blocks of neural network architectures. In general, a convolutional filter applies to the entire frequency spectrum of the input data. We explore artificially constraining the frequency spectra of these filters and data, called band-limiting, during training. The frequency domain constraints apply to both the feed-forward and back-propagation steps. Experimentally, we observe that Convolutional Neural Networks (CNNs) are resilient to this compression scheme and results suggest that CNNs learn to leverage lower-frequency components. In particular, we found: (1) band-limited training can effectively control the resource usage (GPU and memory); (2) models trained with band-limited layers retain high prediction accuracy; and (3) requires no modification to existing training algorithms or neural network architectures to use unlike other compression schemes."
  paper: static/assets/papers/band-limit.pdf
  slides: static/assets/slides/band-limit.pdf
  talk: static/assets/talks/band-limit-video.html
  bibtex: static/assets/abstracts/band-limit.txt

- name: Auto-recommendation of hybrid physical designs
  #i18n: cpython
  url: "https://dl.acm.org/citation.cfm?id=3190660"
  img: static/assets/projects/hybrid-designs3.png
  # desc: Commercial DBMSs, such as Microsoft SQL Server, cater to diverse workloads including transaction processing, decision support, and operational analytics. They also support variety in physical design structures such as B+ tree and columnstore. The benefits of B+ tree for OLTP workloads and columnstore for decision support workloads are well-understood. However, the importance of hybrid physical designs, consisting of both columnstore and B+ tree indexes on the same database, is not well-studied - a focus of this paper. We first quantify the trade-offs using carefully-crafted micro-benchmarks. This micro-benchmarking indicates that hybrid physical designs can result in orders of magnitude better performance depending on the workload. For complex real-world applications, choosing an appropriate combination of columnstore and B+ tree indexes for a database workload is challenging. We extend the Database Engine Tuning Advisor for Microsoft SQL Server to recommend a suitable combination of B+ tree and columnstore indexes for a given workload. Through extensive experiments using industry-standard benchmarks and several real-world customer workloads, we quantify how a physical design tool capable of recommending hybrid physical designs can result in orders of magnitude better execution costs compared to approaches that rely either on columnstore-only or B+ tree-only designs.
  desc: We extend the Database Engine Tuning Advisor for Microsoft SQL Server to recommend a suitable combination of B+ tree and columnstore indexes for a given workload. Through extensive experiments using industry-standard benchmarks and several real-world customer workloads, we quantify how a physical design tool capable of recommending hybrid physical designs can result in orders of magnitude better execution costs compared to approaches that rely either on columnstore-only or B+ tree-only designs.
  paper: static/assets/papers/dziedzic-sigmod2018-recommend-hybrid-designs.pdf
  slides: static/assets/slides/recommend-hybrid-designs-sigmod2018-presentation.pdf
  bibtex: static/assets/abstracts/hybrid_designs.txt

- name: BigDAWG
  #i18n: jalpc
  url: "http://bigdawg.mit.edu"
  gh_user: bigdawg-istc
  repo: bigdawg
  img: static/assets/projects/bigdawg2.png
  desc: An open source project from researchers within the Intel Science and Technology Center for Big Data (ISTC). BigDAWG is a reference implementation of a polystore database. A polystore system is any database management system (DBMS) that is built on top of multiple, heterogeneous, integrated storage engines. I worked on the scaffolding of the system and then implemented a cast operator to move data between diverse DBMSs.
  paper: static/assets/papers/dziedzic_hpec16_data_migration.pdf
  slides: static/assets/slides/data-migration-hpec2016.pdf
  bibtex: static/assets/abstracts/data_migration.txt

- name: Data Loading
  #i18n: cpython
  url: "https://adam-dziedzic.github.io/static/assets/papers/dziedzic_adms16_data_loading.pdf"
  img: static/assets/projects/dataloading2.jpg
  desc: An automated testing infrastructure was built to benchmark the loading performance of several commercial and open-source databases, perform an in-depth analysis to identify bottlenecks of the data loading process and investigate novel techniques which could be used to accelerate DBMS data loading.
  paper: static/assets/papers/dziedzic_adms16_data_loading.pdf
  slides: static/assets/slides/loading-adms-presentation.pdf
  bibtex: static/assets/abstracts/data_loading.txt



